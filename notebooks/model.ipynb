{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o76H5_VvDnf1",
    "outputId": "3f220875-1c80-49bf-a29f-ce5e7111cc2a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# from google.colab import files\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yO4WdP3_VlQP"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OGHQVzp1Mw19"
   },
   "outputs": [],
   "source": [
    "def load_dataset(id):\n",
    "    # folder = '/content/drive/My Drive/_MCGILL/cosmicdawn' # nin\n",
    "#     folder = '/content/drive/My Drive/phys489' # mbj\n",
    "    folder = './data'\n",
    "    data = np.load(f\"{folder}/dataset{id}.npy\")\n",
    "    labels = np.load(f\"{folder}/labels{id}.npy\")\n",
    "    mask = np.load(f\"{folder}/mask{id}.npy\")\n",
    "    # print(data.shape, labels.shape)\n",
    "    return data, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tC2TauSzQ5YB"
   },
   "outputs": [],
   "source": [
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5p6DRBfWLb3e"
   },
   "outputs": [],
   "source": [
    "data, labels, mask = load_dataset(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tep1LJhKK6tA"
   },
   "source": [
    "### Convert to proper dataset shape (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "YagFO444K6-f",
    "outputId": "990eff7c-6e43-4c2a-a9d2-c4b4a832c243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1500, 818, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-29629cf0cae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (20,1500,818,3) into shape (20,1500,818)"
     ]
    }
   ],
   "source": [
    "# real = data.real[:,:]\n",
    "# print(real.shape)\n",
    "# imag = data.imag[:,:,:]\n",
    "# data_new = np.zeros((data.shape[0], data.shape[1], data.shape[2], 3))\n",
    "# data_new[:, :, :, 0] = real\n",
    "# data_new[:, :, :, 1] = imag\n",
    "# data_new[:, :, :, 2] = mask\n",
    "# data = data_new\n",
    "\n",
    "# real = labels.real\n",
    "# imag = labels.imag\n",
    "# labels_new = np.zeros((labels.shape[0], labels.shape[1], labels.shape[2], 2))\n",
    "# labels_new[:, :, :, 0] = real\n",
    "# labels_new[:, :, :, 1] = imag\n",
    "# labels = labels_new\n",
    "\n",
    "# print(data.shape)\n",
    "# print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySlRxMzoK7Pn"
   },
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8Aoap5nFK5Rz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "84xHZBZELthl"
   },
   "outputs": [],
   "source": [
    "# X_train = X_train[:,:,:,:2]\n",
    "# X_test = X_test[:,:,:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-nqIpEFH85e",
    "outputId": "cd64f9d7-6c7d-4e9f-c943-34c934479ddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 1500, 818, 3) (7, 1500, 818, 3) (13, 1500, 818, 2) (7, 1500, 818, 2)\n",
      "(1500, 818)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coYhHWy_VocR"
   },
   "source": [
    "# ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBv7NSG3Ebip"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "'''\n",
    "MSE, only over masked areas\n",
    "'''\n",
    "def masked_MSE(y_true, y_pred):\n",
    "  for yt in y_true: # for each example in the batch\n",
    "    yt = yt[mask == True]\n",
    "  for yp in y_pred:\n",
    "    yp = yp[mask == True]\n",
    "  loss_val = K.mean(K.square(y_pred - y_true))\n",
    "  return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Original deep model\n",
    "'''\n",
    "#   model = keras.Sequential([\n",
    "#       layers.Conv2D(12, kernel_size=3, activation='relu', padding='same', input_shape=(1500,818,3), kernel_initializer=keras.initializers.GlorotNormal()),\n",
    "#       layers.MaxPooling2D((2,2)),\n",
    "#       layers.UpSampling2D((2,2)),\n",
    "#       layers.Conv2D(12, kernel_size=5, activation='relu', padding='same', kernel_initializer=keras.initializers.GlorotNormal()),\n",
    "#       layers.MaxPooling2D((2,2)),\n",
    "#       layers.UpSampling2D((2,2)),\n",
    "#       layers.Conv2D(12, kernel_size=5, activation='relu', padding='same', kernel_initializer=keras.initializers.GlorotNormal()),\n",
    "#       layers.MaxPooling2D((2,2)),\n",
    "#       layers.UpSampling2D((2,2)),\n",
    "#       layers.Conv2D(12, kernel_size=5, activation='relu', padding='same', kernel_initializer=keras.initializers.GlorotNormal()),\n",
    "#       layers.MaxPooling2D((2,2)),\n",
    "#       layers.UpSampling2D((2,2)),\n",
    "#       layers.Dense(12, activation='relu'),\n",
    "#       layers.Dense(2)\n",
    "#   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otz3Ux6-LhlC"
   },
   "outputs": [],
   "source": [
    "def build_and_compile_model():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "#         keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        keras.layers.Dense(4096, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(4096, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(2)\n",
    "    ])\n",
    "    \n",
    "  \n",
    "\n",
    "  model.compile(loss=masked_MSE, optimizer=tf.keras.optimizers.Adam(0.001), metrics=[masked_MSE])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzBzHxunVavn",
    "outputId": "43c8230c-265e-422c-ac9c-4df0e2d02c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_53 (Conv2D)           (None, 1500, 818, 12)     336       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 750, 409, 12)      0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_43 (UpSampling (None, 1500, 818, 12)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 1500, 818, 12)     3612      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 750, 409, 12)      0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_44 (UpSampling (None, 1500, 818, 12)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 1500, 818, 12)     3612      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 750, 409, 12)      0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_45 (UpSampling (None, 1500, 818, 12)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 1500, 818, 12)     3612      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 750, 409, 12)      0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_46 (UpSampling (None, 1500, 818, 12)     0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1500, 818, 12)     156       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1500, 818, 2)      26        \n",
      "=================================================================\n",
      "Total params: 11,354\n",
      "Trainable params: 11,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model = build_and_compile_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXiFCwl3Va_e",
    "outputId": "4f694190-2949-438d-e21a-3d605dc2dcc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 9s 9s/step - loss: 1985.0740 - masked_MSE: 1985.0740 - val_loss: 1848.1903 - val_masked_MSE: 1848.1903\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1863.7406 - masked_MSE: 1863.7407 - val_loss: 1771.3414 - val_masked_MSE: 1771.3416\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1786.7758 - masked_MSE: 1786.7759 - val_loss: 1712.9501 - val_masked_MSE: 1712.9501\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1727.2965 - masked_MSE: 1727.2965 - val_loss: 1661.5436 - val_masked_MSE: 1661.5436\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1672.8336 - masked_MSE: 1672.8336 - val_loss: 1610.0675 - val_masked_MSE: 1610.0675\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1616.6542 - masked_MSE: 1616.6542 - val_loss: 1559.5836 - val_masked_MSE: 1559.5836\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1560.0251 - masked_MSE: 1560.0251 - val_loss: 1519.8942 - val_masked_MSE: 1519.8942\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1513.8477 - masked_MSE: 1513.8477 - val_loss: 1498.7545 - val_masked_MSE: 1498.7546\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1489.5876 - masked_MSE: 1489.5876 - val_loss: 1477.4569 - val_masked_MSE: 1477.4569\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1470.0569 - masked_MSE: 1470.0569 - val_loss: 1444.6465 - val_masked_MSE: 1444.6465\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1440.5630 - masked_MSE: 1440.5630 - val_loss: 1399.3599 - val_masked_MSE: 1399.3599\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1397.6647 - masked_MSE: 1397.6647 - val_loss: 1338.6243 - val_masked_MSE: 1338.6243\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1340.2618 - masked_MSE: 1340.2618 - val_loss: 1267.0343 - val_masked_MSE: 1267.0343\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1275.6287 - masked_MSE: 1275.6287 - val_loss: 1198.9265 - val_masked_MSE: 1198.9265\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1216.6871 - masked_MSE: 1216.6871 - val_loss: 1136.9775 - val_masked_MSE: 1136.9775\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1167.1658 - masked_MSE: 1167.1658 - val_loss: 1079.9542 - val_masked_MSE: 1079.9542\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1123.8364 - masked_MSE: 1123.8364 - val_loss: 1017.2014 - val_masked_MSE: 1017.2013\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1075.4816 - masked_MSE: 1075.4816 - val_loss: 953.0783 - val_masked_MSE: 953.0783\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1027.1239 - masked_MSE: 1027.1239 - val_loss: 917.3914 - val_masked_MSE: 917.3914\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1004.8291 - masked_MSE: 1004.8291 - val_loss: 906.7313 - val_masked_MSE: 906.7313\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1001.0573 - masked_MSE: 1001.0573 - val_loss: 898.7718 - val_masked_MSE: 898.7719\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 991.9906 - masked_MSE: 991.9906 - val_loss: 884.8740 - val_masked_MSE: 884.8740\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 974.3217 - masked_MSE: 974.3217 - val_loss: 869.6600 - val_masked_MSE: 869.6600\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 959.0446 - masked_MSE: 959.0445 - val_loss: 860.8127 - val_masked_MSE: 860.8127\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 953.1949 - masked_MSE: 953.1949 - val_loss: 857.1799 - val_masked_MSE: 857.1799\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 951.4240 - masked_MSE: 951.4240 - val_loss: 856.5042 - val_masked_MSE: 856.5043\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 950.0252 - masked_MSE: 950.0252 - val_loss: 860.2928 - val_masked_MSE: 860.2928\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 951.6066 - masked_MSE: 951.6066 - val_loss: 865.8853 - val_masked_MSE: 865.8853\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 954.5915 - masked_MSE: 954.5915 - val_loss: 868.7126 - val_masked_MSE: 868.7125\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 954.9709 - masked_MSE: 954.9709 - val_loss: 867.2197 - val_masked_MSE: 867.2197\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 951.8723 - masked_MSE: 951.8723 - val_loss: 862.8525 - val_masked_MSE: 862.8525\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 946.8747 - masked_MSE: 946.8747 - val_loss: 859.4191 - val_masked_MSE: 859.4191\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 943.4201 - masked_MSE: 943.4201 - val_loss: 857.3071 - val_masked_MSE: 857.3071\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 941.4584 - masked_MSE: 941.4584 - val_loss: 851.9854 - val_masked_MSE: 851.9854\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 936.7701 - masked_MSE: 936.7701 - val_loss: 844.5685 - val_masked_MSE: 844.5685\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 930.4813 - masked_MSE: 930.4812 - val_loss: 838.4380 - val_masked_MSE: 838.4380\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 925.4949 - masked_MSE: 925.4949 - val_loss: 835.2440 - val_masked_MSE: 835.2440\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 923.1164 - masked_MSE: 923.1164 - val_loss: 834.3405 - val_masked_MSE: 834.3405\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 922.7372 - masked_MSE: 922.7372 - val_loss: 833.9299 - val_masked_MSE: 833.9299\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 922.7643 - masked_MSE: 922.7643 - val_loss: 832.5901 - val_masked_MSE: 832.5901\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 921.9058 - masked_MSE: 921.9058 - val_loss: 830.1253 - val_masked_MSE: 830.1253\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 919.9446 - masked_MSE: 919.9446 - val_loss: 827.4069 - val_masked_MSE: 827.4069\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 917.6649 - masked_MSE: 917.6648 - val_loss: 825.2607 - val_masked_MSE: 825.2607\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 915.8506 - masked_MSE: 915.8506 - val_loss: 823.6891 - val_masked_MSE: 823.6891\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 914.5658 - masked_MSE: 914.5659 - val_loss: 822.3665 - val_masked_MSE: 822.3665\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 913.5960 - masked_MSE: 913.5960 - val_loss: 821.1037 - val_masked_MSE: 821.1037\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 912.7975 - masked_MSE: 912.7975 - val_loss: 819.7066 - val_masked_MSE: 819.7066\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 911.8378 - masked_MSE: 911.8378 - val_loss: 817.9665 - val_masked_MSE: 817.9664\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 910.3459 - masked_MSE: 910.3459 - val_loss: 815.9664 - val_masked_MSE: 815.9664\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 908.3781 - masked_MSE: 908.3781 - val_loss: 814.1231 - val_masked_MSE: 814.1231\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 906.4860 - masked_MSE: 906.4860 - val_loss: 812.8867 - val_masked_MSE: 812.8867\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 905.2712 - masked_MSE: 905.2712 - val_loss: 812.0876 - val_masked_MSE: 812.0875\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 904.6717 - masked_MSE: 904.6716 - val_loss: 811.0266 - val_masked_MSE: 811.0265\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 904.0083 - masked_MSE: 904.0083 - val_loss: 809.4822 - val_masked_MSE: 809.4822\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 902.8878 - masked_MSE: 902.8878 - val_loss: 807.9838 - val_masked_MSE: 807.9838\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 901.5936 - masked_MSE: 901.5937 - val_loss: 806.8397 - val_masked_MSE: 806.8397\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 900.3924 - masked_MSE: 900.3923 - val_loss: 805.7363 - val_masked_MSE: 805.7363\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 899.1212 - masked_MSE: 899.1212 - val_loss: 804.4808 - val_masked_MSE: 804.4808\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 897.7551 - masked_MSE: 897.7551 - val_loss: 803.2875 - val_masked_MSE: 803.2875\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 896.5587 - masked_MSE: 896.5587 - val_loss: 802.2589 - val_masked_MSE: 802.2589\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 895.6400 - masked_MSE: 895.6401 - val_loss: 801.2728 - val_masked_MSE: 801.2728\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 894.8318 - masked_MSE: 894.8318 - val_loss: 800.2276 - val_masked_MSE: 800.2276\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 893.9391 - masked_MSE: 893.9391 - val_loss: 799.1706 - val_masked_MSE: 799.1706\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 892.9554 - masked_MSE: 892.9554 - val_loss: 798.1971 - val_masked_MSE: 798.1971\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 892.0057 - masked_MSE: 892.0058 - val_loss: 797.1893 - val_masked_MSE: 797.1893\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 891.0519 - masked_MSE: 891.0518 - val_loss: 796.0066 - val_masked_MSE: 796.0066\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 890.0018 - masked_MSE: 890.0017 - val_loss: 794.8000 - val_masked_MSE: 794.7999\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 888.9659 - masked_MSE: 888.9659 - val_loss: 793.7634 - val_masked_MSE: 793.7634\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 888.0477 - masked_MSE: 888.0478 - val_loss: 792.7773 - val_masked_MSE: 792.7773\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 887.1086 - masked_MSE: 887.1086 - val_loss: 791.6883 - val_masked_MSE: 791.6883\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 886.0343 - masked_MSE: 886.0342 - val_loss: 790.5849 - val_masked_MSE: 790.5849\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 884.9325 - masked_MSE: 884.9326 - val_loss: 789.5756 - val_masked_MSE: 789.5756\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 883.9238 - masked_MSE: 883.9238 - val_loss: 788.6192 - val_masked_MSE: 788.6192\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 882.9722 - masked_MSE: 882.9722 - val_loss: 787.6550 - val_masked_MSE: 787.6550\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 882.0109 - masked_MSE: 882.0109 - val_loss: 786.6855 - val_masked_MSE: 786.6855\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 881.0314 - masked_MSE: 881.0315 - val_loss: 785.7038 - val_masked_MSE: 785.7037\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 880.0396 - masked_MSE: 880.0396 - val_loss: 784.6687 - val_masked_MSE: 784.6687\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 879.0122 - masked_MSE: 879.0122 - val_loss: 783.5665 - val_masked_MSE: 783.5665\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 877.9474 - masked_MSE: 877.9474 - val_loss: 782.4409 - val_masked_MSE: 782.4409\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 876.8866 - masked_MSE: 876.8867 - val_loss: 781.3196 - val_masked_MSE: 781.3196\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 875.8359 - masked_MSE: 875.8359 - val_loss: 780.2132 - val_masked_MSE: 780.2132\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 874.7658 - masked_MSE: 874.7659 - val_loss: 779.1890 - val_masked_MSE: 779.1890\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 873.7000 - masked_MSE: 873.7000 - val_loss: 778.2580 - val_masked_MSE: 778.2580\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 872.6611 - masked_MSE: 872.6611 - val_loss: 777.3132 - val_masked_MSE: 777.3132\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 871.6003 - masked_MSE: 871.6003 - val_loss: 776.3174 - val_masked_MSE: 776.3174\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 870.5021 - masked_MSE: 870.5021 - val_loss: 775.3459 - val_masked_MSE: 775.3459\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 869.4186 - masked_MSE: 869.4186 - val_loss: 774.4295 - val_masked_MSE: 774.4295\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 868.3634 - masked_MSE: 868.3634 - val_loss: 773.5130 - val_masked_MSE: 773.5130\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 867.3000 - masked_MSE: 867.3000 - val_loss: 772.5807 - val_masked_MSE: 772.5807\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 866.2269 - masked_MSE: 866.2269 - val_loss: 771.6468 - val_masked_MSE: 771.6468\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 865.1606 - masked_MSE: 865.1606 - val_loss: 770.6786 - val_masked_MSE: 770.6786\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 864.0851 - masked_MSE: 864.0851 - val_loss: 769.6784 - val_masked_MSE: 769.6783\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 863.0099 - masked_MSE: 863.0098 - val_loss: 768.6784 - val_masked_MSE: 768.6784\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 861.9312 - masked_MSE: 861.9312 - val_loss: 767.7146 - val_masked_MSE: 767.7146\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 860.8391 - masked_MSE: 860.8391 - val_loss: 766.7975 - val_masked_MSE: 766.7975\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 859.7592 - masked_MSE: 859.7592 - val_loss: 765.8723 - val_masked_MSE: 765.8723\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 858.6883 - masked_MSE: 858.6882 - val_loss: 764.9192 - val_masked_MSE: 764.9192\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 857.6112 - masked_MSE: 857.6113 - val_loss: 763.9811 - val_masked_MSE: 763.9811\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 856.5323 - masked_MSE: 856.5323 - val_loss: 763.0732 - val_masked_MSE: 763.0732\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 855.4525 - masked_MSE: 855.4526 - val_loss: 762.2002 - val_masked_MSE: 762.2002\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 854.3801 - masked_MSE: 854.3801 - val_loss: 761.3546 - val_masked_MSE: 761.3546\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 853.3204 - masked_MSE: 853.3204 - val_loss: 760.5014 - val_masked_MSE: 760.5014\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 852.2570 - masked_MSE: 852.2571 - val_loss: 759.6447 - val_masked_MSE: 759.6447\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 851.1959 - masked_MSE: 851.1959 - val_loss: 758.8177 - val_masked_MSE: 758.8177\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 850.1424 - masked_MSE: 850.1423 - val_loss: 758.0275 - val_masked_MSE: 758.0275\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 849.0966 - masked_MSE: 849.0966 - val_loss: 757.2348 - val_masked_MSE: 757.2348\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 848.0633 - masked_MSE: 848.0633 - val_loss: 756.3940 - val_masked_MSE: 756.3940\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 847.0312 - masked_MSE: 847.0312 - val_loss: 755.5376 - val_masked_MSE: 755.5376\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 846.0062 - masked_MSE: 846.0063 - val_loss: 754.7058 - val_masked_MSE: 754.7058\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 844.9901 - masked_MSE: 844.9901 - val_loss: 753.9040 - val_masked_MSE: 753.9039\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 843.9781 - masked_MSE: 843.9781 - val_loss: 753.1251 - val_masked_MSE: 753.1251\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 842.9763 - masked_MSE: 842.9763 - val_loss: 752.3423 - val_masked_MSE: 752.3423\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 841.9828 - masked_MSE: 841.9828 - val_loss: 751.5526 - val_masked_MSE: 751.5526\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 841.0021 - masked_MSE: 841.0020 - val_loss: 750.7728 - val_masked_MSE: 750.7728\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 840.0341 - masked_MSE: 840.0341 - val_loss: 750.0106 - val_masked_MSE: 750.0106\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 839.0753 - masked_MSE: 839.0753 - val_loss: 749.2516 - val_masked_MSE: 749.2516\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 838.1268 - masked_MSE: 838.1268 - val_loss: 748.4850 - val_masked_MSE: 748.4850\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 837.1896 - masked_MSE: 837.1897 - val_loss: 747.7161 - val_masked_MSE: 747.7162\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 836.2633 - masked_MSE: 836.2632 - val_loss: 746.9619 - val_masked_MSE: 746.9619\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 835.3490 - masked_MSE: 835.3490 - val_loss: 746.2358 - val_masked_MSE: 746.2358\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 834.4471 - masked_MSE: 834.4470 - val_loss: 745.5318 - val_masked_MSE: 745.5318\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 833.5581 - masked_MSE: 833.5581 - val_loss: 744.8259 - val_masked_MSE: 744.8258\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 832.6832 - masked_MSE: 832.6832 - val_loss: 744.1082 - val_masked_MSE: 744.1082\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 831.8260 - masked_MSE: 831.8260 - val_loss: 743.3926 - val_masked_MSE: 743.3926\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 830.9784 - masked_MSE: 830.9784 - val_loss: 742.6895 - val_masked_MSE: 742.6895\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 830.1404 - masked_MSE: 830.1404 - val_loss: 742.0121 - val_masked_MSE: 742.0121\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 829.3143 - masked_MSE: 829.3144 - val_loss: 741.3557 - val_masked_MSE: 741.3557\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 828.5057 - masked_MSE: 828.5056 - val_loss: 740.6992 - val_masked_MSE: 740.6992\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 827.7069 - masked_MSE: 827.7070 - val_loss: 740.0513 - val_masked_MSE: 740.0513\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 826.9221 - masked_MSE: 826.9221 - val_loss: 739.4241 - val_masked_MSE: 739.4241\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 826.1500 - masked_MSE: 826.1500 - val_loss: 738.8168 - val_masked_MSE: 738.8168\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 825.3910 - masked_MSE: 825.3910 - val_loss: 738.2208 - val_masked_MSE: 738.2208\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 824.6434 - masked_MSE: 824.6435 - val_loss: 737.6190 - val_masked_MSE: 737.6190\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 823.9045 - masked_MSE: 823.9045 - val_loss: 737.0215 - val_masked_MSE: 737.0215\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 823.1727 - masked_MSE: 823.1728 - val_loss: 736.4390 - val_masked_MSE: 736.4390\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 822.4502 - masked_MSE: 822.4502 - val_loss: 735.8806 - val_masked_MSE: 735.8805\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 821.7404 - masked_MSE: 821.7404 - val_loss: 735.3401 - val_masked_MSE: 735.3401\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 821.0419 - masked_MSE: 821.0419 - val_loss: 734.8062 - val_masked_MSE: 734.8062\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 820.3495 - masked_MSE: 820.3495 - val_loss: 734.2766 - val_masked_MSE: 734.2765\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 819.6633 - masked_MSE: 819.6633 - val_loss: 733.7339 - val_masked_MSE: 733.7339\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 818.9854 - masked_MSE: 818.9854 - val_loss: 733.1846 - val_masked_MSE: 733.1846\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 818.3151 - masked_MSE: 818.3151 - val_loss: 732.6355 - val_masked_MSE: 732.6355\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 817.6517 - masked_MSE: 817.6517 - val_loss: 732.0874 - val_masked_MSE: 732.0874\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 816.9912 - masked_MSE: 816.9912 - val_loss: 731.5503 - val_masked_MSE: 731.5503\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 816.3329 - masked_MSE: 816.3329 - val_loss: 731.0237 - val_masked_MSE: 731.0237\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 815.6797 - masked_MSE: 815.6797 - val_loss: 730.5063 - val_masked_MSE: 730.5063\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 815.0319 - masked_MSE: 815.0319 - val_loss: 729.9893 - val_masked_MSE: 729.9893\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 814.3877 - masked_MSE: 814.3877 - val_loss: 729.4645 - val_masked_MSE: 729.4645\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 813.7452 - masked_MSE: 813.7452 - val_loss: 728.9350 - val_masked_MSE: 728.9350\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 813.1054 - masked_MSE: 813.1054 - val_loss: 728.4057 - val_masked_MSE: 728.4057\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 812.4706 - masked_MSE: 812.4706 - val_loss: 727.8817 - val_masked_MSE: 727.8817\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 811.8392 - masked_MSE: 811.8392 - val_loss: 727.3499 - val_masked_MSE: 727.3499\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 811.2097 - masked_MSE: 811.2097 - val_loss: 726.8137 - val_masked_MSE: 726.8136\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 810.5836 - masked_MSE: 810.5836 - val_loss: 726.2781 - val_masked_MSE: 726.2781\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 809.9625 - masked_MSE: 809.9625 - val_loss: 725.7477 - val_masked_MSE: 725.7477\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 809.3440 - masked_MSE: 809.3440 - val_loss: 725.2134 - val_masked_MSE: 725.2134\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 808.7277 - masked_MSE: 808.7277 - val_loss: 724.6694 - val_masked_MSE: 724.6694\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 808.1134 - masked_MSE: 808.1134 - val_loss: 724.1153 - val_masked_MSE: 724.1153\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 807.4913 - masked_MSE: 807.4913 - val_loss: 723.6492 - val_masked_MSE: 723.6492\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 806.8356 - masked_MSE: 806.8356 - val_loss: 723.2267 - val_masked_MSE: 723.2267\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 806.1774 - masked_MSE: 806.1774 - val_loss: 722.7048 - val_masked_MSE: 722.7048\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 805.5139 - masked_MSE: 805.5139 - val_loss: 722.0842 - val_masked_MSE: 722.0842\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 804.8404 - masked_MSE: 804.8404 - val_loss: 721.3752 - val_masked_MSE: 721.3751\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 804.1620 - masked_MSE: 804.1620 - val_loss: 720.7520 - val_masked_MSE: 720.7520\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 803.4761 - masked_MSE: 803.4761 - val_loss: 720.1415 - val_masked_MSE: 720.1415\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 802.7714 - masked_MSE: 802.7714 - val_loss: 719.7775 - val_masked_MSE: 719.7775\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 802.0453 - masked_MSE: 802.0453 - val_loss: 719.3015 - val_masked_MSE: 719.3015\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 801.4370 - masked_MSE: 801.4370 - val_loss: 719.3270 - val_masked_MSE: 719.3270\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 801.0682 - masked_MSE: 801.0682 - val_loss: 719.5812 - val_masked_MSE: 719.5812\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 801.3671 - masked_MSE: 801.3671 - val_loss: 721.2788 - val_masked_MSE: 721.2788\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 802.5056 - masked_MSE: 802.5056 - val_loss: 721.0286 - val_masked_MSE: 721.0286\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 802.6483 - masked_MSE: 802.6483 - val_loss: 717.1345 - val_masked_MSE: 717.1345\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 798.6981 - masked_MSE: 798.6981 - val_loss: 713.6424 - val_masked_MSE: 713.6425\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 795.4761 - masked_MSE: 795.4761 - val_loss: 714.6658 - val_masked_MSE: 714.6658\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 796.6843 - masked_MSE: 796.6843 - val_loss: 714.9697 - val_masked_MSE: 714.9697\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 796.6215 - masked_MSE: 796.6215 - val_loss: 711.4888 - val_masked_MSE: 711.4888\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 793.5822 - masked_MSE: 793.5822 - val_loss: 710.9323 - val_masked_MSE: 710.9323\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 793.0321 - masked_MSE: 793.0321 - val_loss: 711.8708 - val_masked_MSE: 711.8708\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 793.6078 - masked_MSE: 793.6078 - val_loss: 709.3369 - val_masked_MSE: 709.3369\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 791.6499 - masked_MSE: 791.6499 - val_loss: 708.4830 - val_masked_MSE: 708.4830\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 790.7675 - masked_MSE: 790.7675 - val_loss: 709.4197 - val_masked_MSE: 709.4198\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 791.0775 - masked_MSE: 791.0775 - val_loss: 707.8320 - val_masked_MSE: 707.8320\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 789.4995 - masked_MSE: 789.4995 - val_loss: 707.2927 - val_masked_MSE: 707.2927\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 788.5132 - masked_MSE: 788.5132 - val_loss: 707.9505 - val_masked_MSE: 707.9505\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 788.5762 - masked_MSE: 788.5762 - val_loss: 706.6145 - val_masked_MSE: 706.6145\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 787.3140 - masked_MSE: 787.3140 - val_loss: 705.9731 - val_masked_MSE: 705.9731\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 786.3275 - masked_MSE: 786.3275 - val_loss: 706.3366 - val_masked_MSE: 706.3366\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 786.2203 - masked_MSE: 786.2203 - val_loss: 705.3190 - val_masked_MSE: 705.3190\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 785.2074 - masked_MSE: 785.2074 - val_loss: 704.8104 - val_masked_MSE: 704.8104\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 784.3054 - masked_MSE: 784.3054 - val_loss: 705.2021 - val_masked_MSE: 705.2021\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 784.0735 - masked_MSE: 784.0735 - val_loss: 704.5620 - val_masked_MSE: 704.5620\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 783.2609 - masked_MSE: 783.2609 - val_loss: 703.8317 - val_masked_MSE: 703.8317\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 782.2297 - masked_MSE: 782.2297 - val_loss: 703.6890 - val_masked_MSE: 703.6890\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 781.8360 - masked_MSE: 781.8360 - val_loss: 703.0994 - val_masked_MSE: 703.0994\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 781.0978 - masked_MSE: 781.0978 - val_loss: 702.4783 - val_masked_MSE: 702.4784\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 780.0356 - masked_MSE: 780.0356 - val_loss: 702.1237 - val_masked_MSE: 702.1237\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 779.3741 - masked_MSE: 779.3741 - val_loss: 701.6893 - val_masked_MSE: 701.6893\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 778.7671 - masked_MSE: 778.7671 - val_loss: 700.9877 - val_masked_MSE: 700.9877\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 777.7911 - masked_MSE: 777.7911 - val_loss: 700.5157 - val_masked_MSE: 700.5157\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 777.0466 - masked_MSE: 777.0466 - val_loss: 700.1728 - val_masked_MSE: 700.1728\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 776.4067 - masked_MSE: 776.4067 - val_loss: 699.8508 - val_masked_MSE: 699.8507\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 775.5042 - masked_MSE: 775.5042 - val_loss: 699.6057 - val_masked_MSE: 699.6057\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 774.6761 - masked_MSE: 774.6761 - val_loss: 699.1584 - val_masked_MSE: 699.1583\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 773.9089 - masked_MSE: 773.9089 - val_loss: 698.6413 - val_masked_MSE: 698.6413\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 773.1631 - masked_MSE: 773.1631 - val_loss: 698.2864 - val_masked_MSE: 698.2864\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 772.1934 - masked_MSE: 772.1934 - val_loss: 698.2463 - val_masked_MSE: 698.2463\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 771.3710 - masked_MSE: 771.3710 - val_loss: 698.0579 - val_masked_MSE: 698.0580\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 770.5521 - masked_MSE: 770.5521 - val_loss: 697.3669 - val_masked_MSE: 697.3669\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 769.4985 - masked_MSE: 769.4985 - val_loss: 697.0275 - val_masked_MSE: 697.0275\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 768.5297 - masked_MSE: 768.5297 - val_loss: 696.6010 - val_masked_MSE: 696.6010\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 767.6536 - masked_MSE: 767.6536 - val_loss: 696.1065 - val_masked_MSE: 696.1065\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 766.5769 - masked_MSE: 766.5769 - val_loss: 695.5735 - val_masked_MSE: 695.5735\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 765.4651 - masked_MSE: 765.4651 - val_loss: 695.1142 - val_masked_MSE: 695.1142\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 764.4296 - masked_MSE: 764.4296 - val_loss: 694.6897 - val_masked_MSE: 694.6897\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 763.3098 - masked_MSE: 763.3098 - val_loss: 694.1448 - val_masked_MSE: 694.1449\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 762.0621 - masked_MSE: 762.0621 - val_loss: 693.9589 - val_masked_MSE: 693.9589\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 760.8096 - masked_MSE: 760.8096 - val_loss: 693.8327 - val_masked_MSE: 693.8327\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 759.5536 - masked_MSE: 759.5536 - val_loss: 693.5101 - val_masked_MSE: 693.5101\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 758.1035 - masked_MSE: 758.1035 - val_loss: 692.9162 - val_masked_MSE: 692.9162\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 756.6559 - masked_MSE: 756.6559 - val_loss: 691.8060 - val_masked_MSE: 691.8060\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 755.1013 - masked_MSE: 755.1013 - val_loss: 691.0491 - val_masked_MSE: 691.0492\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 753.5153 - masked_MSE: 753.5153 - val_loss: 690.5694 - val_masked_MSE: 690.5694\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 751.7908 - masked_MSE: 751.7908 - val_loss: 690.3038 - val_masked_MSE: 690.3038\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 750.0602 - masked_MSE: 750.0602 - val_loss: 689.9909 - val_masked_MSE: 689.9908\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 748.2727 - masked_MSE: 748.2727 - val_loss: 689.6151 - val_masked_MSE: 689.6151\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 746.2926 - masked_MSE: 746.2926 - val_loss: 689.0255 - val_masked_MSE: 689.0255\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 744.2081 - masked_MSE: 744.2081 - val_loss: 688.3533 - val_masked_MSE: 688.3534\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 742.0428 - masked_MSE: 742.0428 - val_loss: 687.6976 - val_masked_MSE: 687.6976\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 739.7526 - masked_MSE: 739.7526 - val_loss: 687.3889 - val_masked_MSE: 687.3889\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 737.2902 - masked_MSE: 737.2902 - val_loss: 687.3460 - val_masked_MSE: 687.3460\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 734.7905 - masked_MSE: 734.7905 - val_loss: 686.5605 - val_masked_MSE: 686.5605\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 732.1478 - masked_MSE: 732.1478 - val_loss: 687.0886 - val_masked_MSE: 687.0886\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 729.6899 - masked_MSE: 729.6899 - val_loss: 686.8831 - val_masked_MSE: 686.8831\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 727.8448 - masked_MSE: 727.8448 - val_loss: 689.1639 - val_masked_MSE: 689.1639\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 726.4676 - masked_MSE: 726.4676 - val_loss: 690.2488 - val_masked_MSE: 690.2488\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 726.1070 - masked_MSE: 726.1070 - val_loss: 691.2769 - val_masked_MSE: 691.2769\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 723.4684 - masked_MSE: 723.4684 - val_loss: 687.9116 - val_masked_MSE: 687.9116\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 718.4328 - masked_MSE: 718.4328 - val_loss: 684.8230 - val_masked_MSE: 684.8230\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 711.9241 - masked_MSE: 711.9241 - val_loss: 684.8813 - val_masked_MSE: 684.8813\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 708.6333 - masked_MSE: 708.6333 - val_loss: 687.6203 - val_masked_MSE: 687.6203\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 708.9620 - masked_MSE: 708.9620 - val_loss: 692.0295 - val_masked_MSE: 692.0294\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 707.7175 - masked_MSE: 707.7175 - val_loss: 688.4126 - val_masked_MSE: 688.4126\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 703.8396 - masked_MSE: 703.8396 - val_loss: 685.5129 - val_masked_MSE: 685.5129\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 695.3922 - masked_MSE: 695.3922 - val_loss: 684.9481 - val_masked_MSE: 684.9481\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 691.1356 - masked_MSE: 691.1356 - val_loss: 686.7573 - val_masked_MSE: 686.7573\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 691.0558 - masked_MSE: 691.0558 - val_loss: 692.6401 - val_masked_MSE: 692.6401\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 689.9210 - masked_MSE: 689.9210 - val_loss: 688.7968 - val_masked_MSE: 688.7968\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 685.3953 - masked_MSE: 685.3953 - val_loss: 687.3292 - val_masked_MSE: 687.3292\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 676.4290 - masked_MSE: 676.4290 - val_loss: 685.9909 - val_masked_MSE: 685.9908\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 671.2781 - masked_MSE: 671.2781 - val_loss: 687.7861 - val_masked_MSE: 687.7861\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 670.4620 - masked_MSE: 670.4620 - val_loss: 697.1243 - val_masked_MSE: 697.1244\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 670.7551 - masked_MSE: 670.7551 - val_loss: 695.0374 - val_masked_MSE: 695.0375\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 668.8590 - masked_MSE: 668.8590 - val_loss: 690.5503 - val_masked_MSE: 690.5503\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 658.7979 - masked_MSE: 658.7979 - val_loss: 695.7991 - val_masked_MSE: 695.7991\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 655.5671 - masked_MSE: 655.5671 - val_loss: 699.2150 - val_masked_MSE: 699.2150\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 653.1599 - masked_MSE: 653.1599 - val_loss: 696.5103 - val_masked_MSE: 696.5103\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 645.6763 - masked_MSE: 645.6763 - val_loss: 696.1067 - val_masked_MSE: 696.1067\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 641.2098 - masked_MSE: 641.2098 - val_loss: 702.1912 - val_masked_MSE: 702.1912\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 640.1018 - masked_MSE: 640.1018 - val_loss: 703.0854 - val_masked_MSE: 703.0854\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 633.2552 - masked_MSE: 633.2552 - val_loss: 708.1608 - val_masked_MSE: 708.1608\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 630.4064 - masked_MSE: 630.4064 - val_loss: 715.3461 - val_masked_MSE: 715.3461\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 629.6497 - masked_MSE: 629.6497 - val_loss: 712.3029 - val_masked_MSE: 712.3029\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 627.0540 - masked_MSE: 627.0540 - val_loss: 719.7986 - val_masked_MSE: 719.7986\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 629.9846 - masked_MSE: 629.9846 - val_loss: 718.9031 - val_masked_MSE: 718.9031\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 621.8919 - masked_MSE: 621.8919 - val_loss: 725.2814 - val_masked_MSE: 725.2814\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 620.2726 - masked_MSE: 620.2726 - val_loss: 734.2734 - val_masked_MSE: 734.2734\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 620.5150 - masked_MSE: 620.5150 - val_loss: 735.0803 - val_masked_MSE: 735.0803\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 613.8710 - masked_MSE: 613.8710 - val_loss: 740.2787 - val_masked_MSE: 740.2787\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 613.2450 - masked_MSE: 613.2450 - val_loss: 775.5276 - val_masked_MSE: 775.5276\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 639.2525 - masked_MSE: 639.2525 - val_loss: 789.5623 - val_masked_MSE: 789.5624\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 670.3844 - masked_MSE: 670.3844 - val_loss: 753.5866 - val_masked_MSE: 753.5865\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 643.7921 - masked_MSE: 643.7921 - val_loss: 756.3459 - val_masked_MSE: 756.3459\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 648.4485 - masked_MSE: 648.4485 - val_loss: 754.4256 - val_masked_MSE: 754.4257\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 645.6378 - masked_MSE: 645.6378 - val_loss: 757.7076 - val_masked_MSE: 757.7076\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 651.0535 - masked_MSE: 651.0535 - val_loss: 756.4211 - val_masked_MSE: 756.4211\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 651.0137 - masked_MSE: 651.0137 - val_loss: 746.4324 - val_masked_MSE: 746.4324\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 639.3846 - masked_MSE: 639.3846 - val_loss: 750.5860 - val_masked_MSE: 750.5860\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 637.3422 - masked_MSE: 637.3422 - val_loss: 743.6766 - val_masked_MSE: 743.6766\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 629.8018 - masked_MSE: 629.8018 - val_loss: 754.4038 - val_masked_MSE: 754.4038\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 633.5145 - masked_MSE: 633.5145 - val_loss: 757.3572 - val_masked_MSE: 757.3572\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 625.8811 - masked_MSE: 625.8811 - val_loss: 767.8525 - val_masked_MSE: 767.8525\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 626.7075 - masked_MSE: 626.7075 - val_loss: 766.1972 - val_masked_MSE: 766.1972\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 621.0795 - masked_MSE: 621.0795 - val_loss: 766.3472 - val_masked_MSE: 766.3472\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 618.3328 - masked_MSE: 618.3328 - val_loss: 770.8532 - val_masked_MSE: 770.8531\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 617.6080 - masked_MSE: 617.6080 - val_loss: 774.2827 - val_masked_MSE: 774.2827\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 615.3903 - masked_MSE: 615.3903 - val_loss: 781.0888 - val_masked_MSE: 781.0888\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 618.0366 - masked_MSE: 618.0366 - val_loss: 773.5539 - val_masked_MSE: 773.5538\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 611.0653 - masked_MSE: 611.0653 - val_loss: 773.7419 - val_masked_MSE: 773.7418\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 612.8148 - masked_MSE: 612.8148 - val_loss: 768.4639 - val_masked_MSE: 768.4639\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 608.2086 - masked_MSE: 608.2086 - val_loss: 769.4045 - val_masked_MSE: 769.4045\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 608.8765 - masked_MSE: 608.8765 - val_loss: 765.7376 - val_masked_MSE: 765.7377\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 606.7924 - masked_MSE: 606.7924 - val_loss: 763.8853 - val_masked_MSE: 763.8853\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 607.3350 - masked_MSE: 607.3350 - val_loss: 761.3613 - val_masked_MSE: 761.3613\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 606.6672 - masked_MSE: 606.6672 - val_loss: 759.9172 - val_masked_MSE: 759.9171\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 605.0726 - masked_MSE: 605.0726 - val_loss: 761.2395 - val_masked_MSE: 761.2395\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 604.4580 - masked_MSE: 604.4580 - val_loss: 760.2233 - val_masked_MSE: 760.2233\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 603.4093 - masked_MSE: 603.4093 - val_loss: 758.0364 - val_masked_MSE: 758.0364\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 602.7997 - masked_MSE: 602.7997 - val_loss: 755.8630 - val_masked_MSE: 755.8630\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 601.2797 - masked_MSE: 601.2797 - val_loss: 756.4737 - val_masked_MSE: 756.4737\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 600.7411 - masked_MSE: 600.7411 - val_loss: 755.8704 - val_masked_MSE: 755.8704\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 599.8589 - masked_MSE: 599.8589 - val_loss: 754.8848 - val_masked_MSE: 754.8848\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 599.7380 - masked_MSE: 599.7380 - val_loss: 753.6821 - val_masked_MSE: 753.6821\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 598.8391 - masked_MSE: 598.8391 - val_loss: 754.6048 - val_masked_MSE: 754.6048\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 598.7098 - masked_MSE: 598.7098 - val_loss: 754.7087 - val_masked_MSE: 754.7087\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 597.9136 - masked_MSE: 597.9136 - val_loss: 753.6108 - val_masked_MSE: 753.6108\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 597.7816 - masked_MSE: 597.7816 - val_loss: 751.0501 - val_masked_MSE: 751.0502\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 597.2507 - masked_MSE: 597.2507 - val_loss: 750.1450 - val_masked_MSE: 750.1450\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 597.0403 - masked_MSE: 597.0403 - val_loss: 749.8013 - val_masked_MSE: 749.8013\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 596.4254 - masked_MSE: 596.4254 - val_loss: 749.2874 - val_masked_MSE: 749.2875\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 596.3167 - masked_MSE: 596.3167 - val_loss: 747.6965 - val_masked_MSE: 747.6965\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 595.8810 - masked_MSE: 595.8810 - val_loss: 746.7065 - val_masked_MSE: 746.7065\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 595.5807 - masked_MSE: 595.5807 - val_loss: 746.5528 - val_masked_MSE: 746.5528\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 595.2723 - masked_MSE: 595.2723 - val_loss: 746.8947 - val_masked_MSE: 746.8947\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 595.6646 - masked_MSE: 595.6646 - val_loss: 743.5986 - val_masked_MSE: 743.5986\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 595.7762 - masked_MSE: 595.7762 - val_loss: 743.5379 - val_masked_MSE: 743.5380\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 596.0107 - masked_MSE: 596.0107 - val_loss: 744.9205 - val_masked_MSE: 744.9205\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 595.7344 - masked_MSE: 595.7344 - val_loss: 745.3234 - val_masked_MSE: 745.3234\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 594.9321 - masked_MSE: 594.9321 - val_loss: 745.6434 - val_masked_MSE: 745.6434\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 594.4616 - masked_MSE: 594.4616 - val_loss: 747.2556 - val_masked_MSE: 747.2555\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 594.3764 - masked_MSE: 594.3764 - val_loss: 749.2601 - val_masked_MSE: 749.2601\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 594.0842 - masked_MSE: 594.0842 - val_loss: 750.7578 - val_masked_MSE: 750.7578\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 593.9299 - masked_MSE: 593.9299 - val_loss: 750.8830 - val_masked_MSE: 750.8829\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 593.4620 - masked_MSE: 593.4620 - val_loss: 751.3494 - val_masked_MSE: 751.3494\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 593.3111 - masked_MSE: 593.3111 - val_loss: 752.2297 - val_masked_MSE: 752.2296\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 593.0788 - masked_MSE: 593.0788 - val_loss: 752.7023 - val_masked_MSE: 752.7023\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 593.0910 - masked_MSE: 593.0910 - val_loss: 751.3353 - val_masked_MSE: 751.3353\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 592.6266 - masked_MSE: 592.6266 - val_loss: 749.7938 - val_masked_MSE: 749.7938\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 592.3071 - masked_MSE: 592.3071 - val_loss: 748.9670 - val_masked_MSE: 748.9669\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 592.0030 - masked_MSE: 592.0030 - val_loss: 748.9446 - val_masked_MSE: 748.9446\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 592.0043 - masked_MSE: 592.0043 - val_loss: 748.6785 - val_masked_MSE: 748.6786\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 591.8504 - masked_MSE: 591.8504 - val_loss: 748.2883 - val_masked_MSE: 748.2883\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 591.6672 - masked_MSE: 591.6672 - val_loss: 748.0088 - val_masked_MSE: 748.0088\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 591.3253 - masked_MSE: 591.3253 - val_loss: 748.1506 - val_masked_MSE: 748.1506\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 591.1005 - masked_MSE: 591.1005 - val_loss: 748.1746 - val_masked_MSE: 748.1747\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 590.9289 - masked_MSE: 590.9289 - val_loss: 748.0423 - val_masked_MSE: 748.0424\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 590.8029 - masked_MSE: 590.8029 - val_loss: 748.1042 - val_masked_MSE: 748.1042\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 590.6002 - masked_MSE: 590.6002 - val_loss: 748.5280 - val_masked_MSE: 748.5280\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 590.4227 - masked_MSE: 590.4227 - val_loss: 748.6995 - val_masked_MSE: 748.6995\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 590.2506 - masked_MSE: 590.2506 - val_loss: 748.0165 - val_masked_MSE: 748.0165\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 590.0910 - masked_MSE: 590.0910 - val_loss: 747.2546 - val_masked_MSE: 747.2545\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 589.8887 - masked_MSE: 589.8887 - val_loss: 746.8255 - val_masked_MSE: 746.8256\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 589.7494 - masked_MSE: 589.7494 - val_loss: 746.4448 - val_masked_MSE: 746.4448\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 589.5735 - masked_MSE: 589.5735 - val_loss: 746.1087 - val_masked_MSE: 746.1087\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 589.4319 - masked_MSE: 589.4319 - val_loss: 745.9747 - val_masked_MSE: 745.9747\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 589.2701 - masked_MSE: 589.2701 - val_loss: 746.1398 - val_masked_MSE: 746.1398\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 589.1293 - masked_MSE: 589.1293 - val_loss: 746.2747 - val_masked_MSE: 746.2748\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 588.9575 - masked_MSE: 588.9575 - val_loss: 746.3012 - val_masked_MSE: 746.3012\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 588.8269 - masked_MSE: 588.8269 - val_loss: 746.3591 - val_masked_MSE: 746.3590\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 588.6896 - masked_MSE: 588.6896 - val_loss: 746.1276 - val_masked_MSE: 746.1276\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 588.5365 - masked_MSE: 588.5365 - val_loss: 746.2536 - val_masked_MSE: 746.2535\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 588.3804 - masked_MSE: 588.3804 - val_loss: 746.2966 - val_masked_MSE: 746.2966\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 588.2519 - masked_MSE: 588.2519 - val_loss: 745.9955 - val_masked_MSE: 745.9955\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 588.1046 - masked_MSE: 588.1046 - val_loss: 745.7889 - val_masked_MSE: 745.7889\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 587.9650 - masked_MSE: 587.9650 - val_loss: 745.7867 - val_masked_MSE: 745.7867\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 587.8237 - masked_MSE: 587.8237 - val_loss: 745.8164 - val_masked_MSE: 745.8164\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 587.6973 - masked_MSE: 587.6973 - val_loss: 745.7775 - val_masked_MSE: 745.7775\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 587.5562 - masked_MSE: 587.5562 - val_loss: 745.8640 - val_masked_MSE: 745.8640\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 587.4245 - masked_MSE: 587.4245 - val_loss: 746.0724 - val_masked_MSE: 746.0724\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 587.2889 - masked_MSE: 587.2889 - val_loss: 746.1445 - val_masked_MSE: 746.1445\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 587.1655 - masked_MSE: 587.1655 - val_loss: 745.9881 - val_masked_MSE: 745.9882\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 587.0355 - masked_MSE: 587.0355 - val_loss: 745.8881 - val_masked_MSE: 745.8881\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 586.9113 - masked_MSE: 586.9113 - val_loss: 746.0051 - val_masked_MSE: 746.0051\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 586.7778 - masked_MSE: 586.7778 - val_loss: 746.1497 - val_masked_MSE: 746.1498\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 586.6544 - masked_MSE: 586.6544 - val_loss: 746.0799 - val_masked_MSE: 746.0800\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 586.5286 - masked_MSE: 586.5286 - val_loss: 745.9856 - val_masked_MSE: 745.9856\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 586.4093 - masked_MSE: 586.4093 - val_loss: 745.9995 - val_masked_MSE: 745.9995\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 586.2844 - masked_MSE: 586.2844 - val_loss: 745.9825 - val_masked_MSE: 745.9825\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 586.1628 - masked_MSE: 586.1628 - val_loss: 745.8420 - val_masked_MSE: 745.8420\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 586.0400 - masked_MSE: 586.0400 - val_loss: 745.7421 - val_masked_MSE: 745.7421\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 585.9210 - masked_MSE: 585.9210 - val_loss: 745.7302 - val_masked_MSE: 745.7302\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 585.7997 - masked_MSE: 585.7997 - val_loss: 745.6673 - val_masked_MSE: 745.6673\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 585.6817 - masked_MSE: 585.6817 - val_loss: 745.4692 - val_masked_MSE: 745.4692\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 585.5620 - masked_MSE: 585.5620 - val_loss: 745.2869 - val_masked_MSE: 745.2869\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 585.4452 - masked_MSE: 585.4452 - val_loss: 745.1807 - val_masked_MSE: 745.1807\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 585.3273 - masked_MSE: 585.3273 - val_loss: 745.0405 - val_masked_MSE: 745.0405\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 585.2112 - masked_MSE: 585.2112 - val_loss: 744.8279 - val_masked_MSE: 744.8279\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 585.0949 - masked_MSE: 585.0949 - val_loss: 744.6651 - val_masked_MSE: 744.6651\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 584.9818 - masked_MSE: 584.9818 - val_loss: 744.5522 - val_masked_MSE: 744.5522\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 584.8682 - masked_MSE: 584.8682 - val_loss: 744.4055 - val_masked_MSE: 744.4055\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 584.7556 - masked_MSE: 584.7556 - val_loss: 744.2361 - val_masked_MSE: 744.2361\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 584.6433 - masked_MSE: 584.6433 - val_loss: 744.1397 - val_masked_MSE: 744.1397\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 584.5328 - masked_MSE: 584.5328 - val_loss: 744.1065 - val_masked_MSE: 744.1065\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 584.4230 - masked_MSE: 584.4230 - val_loss: 744.0227 - val_masked_MSE: 744.0227\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 584.3141 - masked_MSE: 584.3141 - val_loss: 743.8786 - val_masked_MSE: 743.8785\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 584.2062 - masked_MSE: 584.2062 - val_loss: 743.7615 - val_masked_MSE: 743.7615\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 584.0995 - masked_MSE: 584.0995 - val_loss: 743.6973 - val_masked_MSE: 743.6973\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 583.9935 - masked_MSE: 583.9935 - val_loss: 743.6312 - val_masked_MSE: 743.6312\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 583.8881 - masked_MSE: 583.8881 - val_loss: 743.5759 - val_masked_MSE: 743.5758\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 583.7843 - masked_MSE: 583.7843 - val_loss: 743.5410 - val_masked_MSE: 743.5410\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 583.6810 - masked_MSE: 583.6810 - val_loss: 743.4649 - val_masked_MSE: 743.4649\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 583.5783 - masked_MSE: 583.5783 - val_loss: 743.3555 - val_masked_MSE: 743.3555\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 583.4764 - masked_MSE: 583.4764 - val_loss: 743.2844 - val_masked_MSE: 743.2843\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 583.3754 - masked_MSE: 583.3754 - val_loss: 743.2785 - val_masked_MSE: 743.2785\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 583.2745 - masked_MSE: 583.2745 - val_loss: 743.2282 - val_masked_MSE: 743.2282\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 583.1747 - masked_MSE: 583.1747 - val_loss: 743.1312 - val_masked_MSE: 743.1312\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 583.0760 - masked_MSE: 583.0760 - val_loss: 743.0562 - val_masked_MSE: 743.0562\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.9775 - masked_MSE: 582.9775 - val_loss: 742.9888 - val_masked_MSE: 742.9888\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.8795 - masked_MSE: 582.8795 - val_loss: 742.9075 - val_masked_MSE: 742.9075\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.7822 - masked_MSE: 582.7822 - val_loss: 742.8493 - val_masked_MSE: 742.8492\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.6857 - masked_MSE: 582.6857 - val_loss: 742.8157 - val_masked_MSE: 742.8158\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.5896 - masked_MSE: 582.5896 - val_loss: 742.7563 - val_masked_MSE: 742.7563\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.4941 - masked_MSE: 582.4941 - val_loss: 742.6661 - val_masked_MSE: 742.6661\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.3995 - masked_MSE: 582.3995 - val_loss: 742.6187 - val_masked_MSE: 742.6187\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.3052 - masked_MSE: 582.3052 - val_loss: 742.5927 - val_masked_MSE: 742.5927\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.2125 - masked_MSE: 582.2125 - val_loss: 742.4866 - val_masked_MSE: 742.4866\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.1194 - masked_MSE: 582.1194 - val_loss: 742.3774 - val_masked_MSE: 742.3774\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.0267 - masked_MSE: 582.0267 - val_loss: 742.3074 - val_masked_MSE: 742.3074\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.9329 - masked_MSE: 581.9329 - val_loss: 742.1992 - val_masked_MSE: 742.1992\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.8417 - masked_MSE: 581.8417 - val_loss: 741.9562 - val_masked_MSE: 741.9562\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.7545 - masked_MSE: 581.7545 - val_loss: 741.9020 - val_masked_MSE: 741.9020\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.6644 - masked_MSE: 581.6644 - val_loss: 741.8824 - val_masked_MSE: 741.8824\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.5746 - masked_MSE: 581.5746 - val_loss: 741.7907 - val_masked_MSE: 741.7907\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.4840 - masked_MSE: 581.4840 - val_loss: 741.7518 - val_masked_MSE: 741.7518\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.3950 - masked_MSE: 581.3950 - val_loss: 741.7971 - val_masked_MSE: 741.7971\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.3069 - masked_MSE: 581.3069 - val_loss: 741.7929 - val_masked_MSE: 741.7929\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.2180 - masked_MSE: 581.2180 - val_loss: 741.7504 - val_masked_MSE: 741.7504\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.1293 - masked_MSE: 581.1293 - val_loss: 743.2012 - val_masked_MSE: 743.2012\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.9414 - masked_MSE: 581.9414 - val_loss: 740.0297 - val_masked_MSE: 740.0297\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 583.1132 - masked_MSE: 583.1132 - val_loss: 739.2900 - val_masked_MSE: 739.2900\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 583.2399 - masked_MSE: 583.2399 - val_loss: 739.9232 - val_masked_MSE: 739.9232\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.7842 - masked_MSE: 582.7842 - val_loss: 740.4258 - val_masked_MSE: 740.4258\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.5119 - masked_MSE: 582.5119 - val_loss: 740.8496 - val_masked_MSE: 740.8496\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.7164 - masked_MSE: 581.7164 - val_loss: 742.6829 - val_masked_MSE: 742.6830\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.3265 - masked_MSE: 582.3265 - val_loss: 742.9468 - val_masked_MSE: 742.9468\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.9774 - masked_MSE: 581.9774 - val_loss: 742.8757 - val_masked_MSE: 742.8756\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.2117 - masked_MSE: 581.2117 - val_loss: 744.0396 - val_masked_MSE: 744.0396\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.2238 - masked_MSE: 581.2238 - val_loss: 744.5845 - val_masked_MSE: 744.5845\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.2256 - masked_MSE: 581.2256 - val_loss: 743.9418 - val_masked_MSE: 743.9418\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.3158 - masked_MSE: 581.3158 - val_loss: 742.2742 - val_masked_MSE: 742.2742\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 580.8809 - masked_MSE: 580.8809 - val_loss: 741.0726 - val_masked_MSE: 741.0726\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 580.5445 - masked_MSE: 580.5445 - val_loss: 741.5896 - val_masked_MSE: 741.5896\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 580.6206 - masked_MSE: 580.6206 - val_loss: 741.9885 - val_masked_MSE: 741.9885\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 580.4597 - masked_MSE: 580.4597 - val_loss: 741.6530 - val_masked_MSE: 741.6530\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 580.3828 - masked_MSE: 580.3828 - val_loss: 741.0093 - val_masked_MSE: 741.0093\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 580.1304 - masked_MSE: 580.1304 - val_loss: 740.6092 - val_masked_MSE: 740.6092\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 579.9733 - masked_MSE: 579.9733 - val_loss: 740.6875 - val_masked_MSE: 740.6875\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 579.9302 - masked_MSE: 579.9302 - val_loss: 741.4241 - val_masked_MSE: 741.4241\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 579.7456 - masked_MSE: 579.7456 - val_loss: 742.4911 - val_masked_MSE: 742.4911\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 579.5903 - masked_MSE: 579.5903 - val_loss: 742.6806 - val_masked_MSE: 742.6806\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 579.5663 - masked_MSE: 579.5663 - val_loss: 741.7772 - val_masked_MSE: 741.7772\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 579.4865 - masked_MSE: 579.4865 - val_loss: 741.3018 - val_masked_MSE: 741.3018\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 579.3754 - masked_MSE: 579.3754 - val_loss: 741.0991 - val_masked_MSE: 741.0991\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 579.2834 - masked_MSE: 579.2834 - val_loss: 740.8465 - val_masked_MSE: 740.8466\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 579.1521 - masked_MSE: 579.1521 - val_loss: 740.5544 - val_masked_MSE: 740.5543\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 579.0641 - masked_MSE: 579.0641 - val_loss: 739.9445 - val_masked_MSE: 739.9445\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.9313 - masked_MSE: 578.9313 - val_loss: 739.1951 - val_masked_MSE: 739.1951\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.8646 - masked_MSE: 578.8646 - val_loss: 739.1357 - val_masked_MSE: 739.1357\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.8338 - masked_MSE: 578.8338 - val_loss: 739.7004 - val_masked_MSE: 739.7004\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.7186 - masked_MSE: 578.7186 - val_loss: 739.7741 - val_masked_MSE: 739.7740\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.6090 - masked_MSE: 578.6090 - val_loss: 739.2321 - val_masked_MSE: 739.2321\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.5387 - masked_MSE: 578.5387 - val_loss: 738.9230 - val_masked_MSE: 738.9230\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.4788 - masked_MSE: 578.4788 - val_loss: 739.0671 - val_masked_MSE: 739.0671\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.3894 - masked_MSE: 578.3894 - val_loss: 739.2420 - val_masked_MSE: 739.2421\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.2973 - masked_MSE: 578.2973 - val_loss: 739.3455 - val_masked_MSE: 739.3455\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.2217 - masked_MSE: 578.2217 - val_loss: 739.3057 - val_masked_MSE: 739.3057\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.1542 - masked_MSE: 578.1542 - val_loss: 739.0179 - val_masked_MSE: 739.0179\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.0745 - masked_MSE: 578.0745 - val_loss: 738.9407 - val_masked_MSE: 738.9408\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.9797 - masked_MSE: 577.9797 - val_loss: 739.1736 - val_masked_MSE: 739.1736\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.9214 - masked_MSE: 577.9214 - val_loss: 739.3704 - val_masked_MSE: 739.3704\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.8726 - masked_MSE: 577.8726 - val_loss: 739.2164 - val_masked_MSE: 739.2164\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.7814 - masked_MSE: 577.7814 - val_loss: 739.2693 - val_masked_MSE: 739.2693\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.6978 - masked_MSE: 577.6978 - val_loss: 739.6189 - val_masked_MSE: 739.6189\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.6418 - masked_MSE: 577.6418 - val_loss: 739.8658 - val_masked_MSE: 739.8658\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.5682 - masked_MSE: 577.5682 - val_loss: 739.9988 - val_masked_MSE: 739.9988\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.4966 - masked_MSE: 577.4966 - val_loss: 740.0397 - val_masked_MSE: 740.0397\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.4260 - masked_MSE: 577.4260 - val_loss: 739.8997 - val_masked_MSE: 739.8998\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.3599 - masked_MSE: 577.3599 - val_loss: 739.7092 - val_masked_MSE: 739.7091\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.2951 - masked_MSE: 577.2951 - val_loss: 739.7217 - val_masked_MSE: 739.7217\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.2245 - masked_MSE: 577.2245 - val_loss: 739.8079 - val_masked_MSE: 739.8080\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.1575 - masked_MSE: 577.1575 - val_loss: 739.7252 - val_masked_MSE: 739.7252\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.0894 - masked_MSE: 577.0894 - val_loss: 739.6128 - val_masked_MSE: 739.6128\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.0308 - masked_MSE: 577.0308 - val_loss: 739.5983 - val_masked_MSE: 739.5983\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.9610 - masked_MSE: 576.9610 - val_loss: 739.5471 - val_masked_MSE: 739.5471\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.8923 - masked_MSE: 576.8923 - val_loss: 739.6425 - val_masked_MSE: 739.6425\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.8292 - masked_MSE: 576.8292 - val_loss: 739.7686 - val_masked_MSE: 739.7686\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.7693 - masked_MSE: 576.7693 - val_loss: 739.6750 - val_masked_MSE: 739.6749\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.7003 - masked_MSE: 576.7003 - val_loss: 739.5219 - val_masked_MSE: 739.5219\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.6369 - masked_MSE: 576.6369 - val_loss: 739.4248 - val_masked_MSE: 739.4248\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.5748 - masked_MSE: 576.5748 - val_loss: 739.3211 - val_masked_MSE: 739.3212\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.5105 - masked_MSE: 576.5105 - val_loss: 739.2274 - val_masked_MSE: 739.2274\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.4457 - masked_MSE: 576.4457 - val_loss: 739.1071 - val_masked_MSE: 739.1071\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.3833 - masked_MSE: 576.3833 - val_loss: 738.9709 - val_masked_MSE: 738.9709\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.3212 - masked_MSE: 576.3212 - val_loss: 738.8900 - val_masked_MSE: 738.8900\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.2603 - masked_MSE: 576.2603 - val_loss: 738.9879 - val_masked_MSE: 738.9879\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.1969 - masked_MSE: 576.1969 - val_loss: 739.0656 - val_masked_MSE: 739.0656\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.1398 - masked_MSE: 576.1398 - val_loss: 738.9202 - val_masked_MSE: 738.9202\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.0776 - masked_MSE: 576.0776 - val_loss: 738.6081 - val_masked_MSE: 738.6080\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 576.0144 - masked_MSE: 576.0144 - val_loss: 738.4417 - val_masked_MSE: 738.4418\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.9560 - masked_MSE: 575.9560 - val_loss: 738.4185 - val_masked_MSE: 738.4185\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.8950 - masked_MSE: 575.8950 - val_loss: 738.4240 - val_masked_MSE: 738.4240\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.8340 - masked_MSE: 575.8340 - val_loss: 738.4409 - val_masked_MSE: 738.4409\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.7742 - masked_MSE: 575.7742 - val_loss: 738.2858 - val_masked_MSE: 738.2858\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.7146 - masked_MSE: 575.7146 - val_loss: 738.1092 - val_masked_MSE: 738.1092\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.6563 - masked_MSE: 575.6563 - val_loss: 738.1011 - val_masked_MSE: 738.1011\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.5961 - masked_MSE: 575.5961 - val_loss: 738.1648 - val_masked_MSE: 738.1648\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.5368 - masked_MSE: 575.5368 - val_loss: 738.0845 - val_masked_MSE: 738.0845\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.4793 - masked_MSE: 575.4793 - val_loss: 738.0374 - val_masked_MSE: 738.0374\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.4199 - masked_MSE: 575.4199 - val_loss: 737.9772 - val_masked_MSE: 737.9772\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.3618 - masked_MSE: 575.3618 - val_loss: 737.9880 - val_masked_MSE: 737.9880\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.3053 - masked_MSE: 575.3053 - val_loss: 738.0182 - val_masked_MSE: 738.0182\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.2480 - masked_MSE: 575.2480 - val_loss: 737.9246 - val_masked_MSE: 737.9246\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.1904 - masked_MSE: 575.1904 - val_loss: 737.7645 - val_masked_MSE: 737.7645\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.1326 - masked_MSE: 575.1326 - val_loss: 737.5743 - val_masked_MSE: 737.5743\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.0772 - masked_MSE: 575.0772 - val_loss: 737.4664 - val_masked_MSE: 737.4664\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.0214 - masked_MSE: 575.0214 - val_loss: 737.4622 - val_masked_MSE: 737.4622\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.9648 - masked_MSE: 574.9648 - val_loss: 737.5118 - val_masked_MSE: 737.5118\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.9088 - masked_MSE: 574.9088 - val_loss: 737.5472 - val_masked_MSE: 737.5472\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.8527 - masked_MSE: 574.8527 - val_loss: 737.4637 - val_masked_MSE: 737.4637\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.7977 - masked_MSE: 574.7977 - val_loss: 737.3610 - val_masked_MSE: 737.3610\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.7427 - masked_MSE: 574.7427 - val_loss: 737.3580 - val_masked_MSE: 737.3580\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.6873 - masked_MSE: 574.6873 - val_loss: 737.2713 - val_masked_MSE: 737.2713\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.6332 - masked_MSE: 574.6332 - val_loss: 737.1751 - val_masked_MSE: 737.1751\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.5789 - masked_MSE: 574.5789 - val_loss: 737.1235 - val_masked_MSE: 737.1235\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.5244 - masked_MSE: 574.5244 - val_loss: 737.0945 - val_masked_MSE: 737.0945\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.4702 - masked_MSE: 574.4702 - val_loss: 737.1157 - val_masked_MSE: 737.1157\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.4169 - masked_MSE: 574.4169 - val_loss: 737.1069 - val_masked_MSE: 737.1069\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.3641 - masked_MSE: 574.3641 - val_loss: 737.0391 - val_masked_MSE: 737.0391\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.3112 - masked_MSE: 574.3112 - val_loss: 736.8997 - val_masked_MSE: 736.8997\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.2575 - masked_MSE: 574.2575 - val_loss: 736.7896 - val_masked_MSE: 736.7896\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.2048 - masked_MSE: 574.2048 - val_loss: 736.7634 - val_masked_MSE: 736.7634\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.1523 - masked_MSE: 574.1523 - val_loss: 736.8038 - val_masked_MSE: 736.8038\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.1002 - masked_MSE: 574.1002 - val_loss: 736.8437 - val_masked_MSE: 736.8437\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.0482 - masked_MSE: 574.0482 - val_loss: 736.7730 - val_masked_MSE: 736.7730\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.9958 - masked_MSE: 573.9958 - val_loss: 736.6086 - val_masked_MSE: 736.6085\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.9449 - masked_MSE: 573.9449 - val_loss: 736.5806 - val_masked_MSE: 736.5806\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.8937 - masked_MSE: 573.8937 - val_loss: 736.6370 - val_masked_MSE: 736.6370\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.8424 - masked_MSE: 573.8424 - val_loss: 736.6183 - val_masked_MSE: 736.6183\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.7921 - masked_MSE: 573.7921 - val_loss: 736.5506 - val_masked_MSE: 736.5507\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.7408 - masked_MSE: 573.7408 - val_loss: 736.4480 - val_masked_MSE: 736.4480\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.6908 - masked_MSE: 573.6908 - val_loss: 736.4280 - val_masked_MSE: 736.4280\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.6401 - masked_MSE: 573.6401 - val_loss: 736.4937 - val_masked_MSE: 736.4937\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.5910 - masked_MSE: 573.5910 - val_loss: 736.4085 - val_masked_MSE: 736.4085\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.5408 - masked_MSE: 573.5408 - val_loss: 736.1900 - val_masked_MSE: 736.1900\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.4918 - masked_MSE: 573.4918 - val_loss: 736.1363 - val_masked_MSE: 736.1363\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.4431 - masked_MSE: 573.4431 - val_loss: 736.1883 - val_masked_MSE: 736.1884\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.3934 - masked_MSE: 573.3934 - val_loss: 736.3044 - val_masked_MSE: 736.3043\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.3450 - masked_MSE: 573.3450 - val_loss: 736.2560 - val_masked_MSE: 736.2560\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.2960 - masked_MSE: 573.2960 - val_loss: 736.1135 - val_masked_MSE: 736.1134\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.2477 - masked_MSE: 573.2477 - val_loss: 736.0106 - val_masked_MSE: 736.0106\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.2003 - masked_MSE: 573.2003 - val_loss: 736.0676 - val_masked_MSE: 736.0676\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.1517 - masked_MSE: 573.1517 - val_loss: 736.1573 - val_masked_MSE: 736.1573\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.1044 - masked_MSE: 573.1044 - val_loss: 736.0692 - val_masked_MSE: 736.0692\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.0561 - masked_MSE: 573.0561 - val_loss: 735.9993 - val_masked_MSE: 735.9994\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.0091 - masked_MSE: 573.0091 - val_loss: 735.9974 - val_masked_MSE: 735.9974\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.9625 - masked_MSE: 572.9625 - val_loss: 735.9643 - val_masked_MSE: 735.9643\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.9151 - masked_MSE: 572.9151 - val_loss: 735.9133 - val_masked_MSE: 735.9133\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.8682 - masked_MSE: 572.8682 - val_loss: 735.7542 - val_masked_MSE: 735.7542\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.8221 - masked_MSE: 572.8221 - val_loss: 735.6879 - val_masked_MSE: 735.6879\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.7761 - masked_MSE: 572.7761 - val_loss: 735.7422 - val_masked_MSE: 735.7422\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.7296 - masked_MSE: 572.7296 - val_loss: 735.8049 - val_masked_MSE: 735.8049\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.6832 - masked_MSE: 572.6832 - val_loss: 735.8516 - val_masked_MSE: 735.8516\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.6379 - masked_MSE: 572.6379 - val_loss: 735.7907 - val_masked_MSE: 735.7906\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.5924 - masked_MSE: 572.5924 - val_loss: 735.6995 - val_masked_MSE: 735.6995\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.5468 - masked_MSE: 572.5468 - val_loss: 735.6276 - val_masked_MSE: 735.6276\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.5010 - masked_MSE: 572.5010 - val_loss: 735.5842 - val_masked_MSE: 735.5841\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.4562 - masked_MSE: 572.4562 - val_loss: 735.5612 - val_masked_MSE: 735.5612\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.4113 - masked_MSE: 572.4113 - val_loss: 735.5203 - val_masked_MSE: 735.5204\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.3661 - masked_MSE: 572.3661 - val_loss: 735.5045 - val_masked_MSE: 735.5045\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.3221 - masked_MSE: 572.3221 - val_loss: 735.4797 - val_masked_MSE: 735.4796\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.2776 - masked_MSE: 572.2776 - val_loss: 735.4234 - val_masked_MSE: 735.4234\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.2329 - masked_MSE: 572.2329 - val_loss: 735.4041 - val_masked_MSE: 735.4042\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.1891 - masked_MSE: 572.1891 - val_loss: 735.3647 - val_masked_MSE: 735.3647\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 572.1448 - masked_MSE: 572.1448 - val_loss: 735.3184 - val_masked_MSE: 735.3184\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.1012 - masked_MSE: 572.1012 - val_loss: 735.2178 - val_masked_MSE: 735.2178\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.0578 - masked_MSE: 572.0578 - val_loss: 735.2229 - val_masked_MSE: 735.2229\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.0146 - masked_MSE: 572.0146 - val_loss: 735.2078 - val_masked_MSE: 735.2079\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.9710 - masked_MSE: 571.9710 - val_loss: 735.0915 - val_masked_MSE: 735.0915\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.9280 - masked_MSE: 571.9280 - val_loss: 735.0717 - val_masked_MSE: 735.0717\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.8851 - masked_MSE: 571.8851 - val_loss: 735.0945 - val_masked_MSE: 735.0946\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.8418 - masked_MSE: 571.8418 - val_loss: 735.0682 - val_masked_MSE: 735.0682\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.7997 - masked_MSE: 571.7997 - val_loss: 735.0089 - val_masked_MSE: 735.0089\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.7565 - masked_MSE: 571.7565 - val_loss: 735.0013 - val_masked_MSE: 735.0013\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.7147 - masked_MSE: 571.7147 - val_loss: 734.9184 - val_masked_MSE: 734.9184\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.6725 - masked_MSE: 571.6725 - val_loss: 734.9012 - val_masked_MSE: 734.9012\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.6295 - masked_MSE: 571.6295 - val_loss: 734.9094 - val_masked_MSE: 734.9094\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.5887 - masked_MSE: 571.5887 - val_loss: 734.8602 - val_masked_MSE: 734.8602\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.5461 - masked_MSE: 571.5461 - val_loss: 734.8004 - val_masked_MSE: 734.8004\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.5043 - masked_MSE: 571.5043 - val_loss: 734.7939 - val_masked_MSE: 734.7939\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.4631 - masked_MSE: 571.4631 - val_loss: 734.7297 - val_masked_MSE: 734.7297\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.4210 - masked_MSE: 571.4210 - val_loss: 734.6738 - val_masked_MSE: 734.6738\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.3804 - masked_MSE: 571.3804 - val_loss: 734.6624 - val_masked_MSE: 734.6624\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.3387 - masked_MSE: 571.3387 - val_loss: 734.6863 - val_masked_MSE: 734.6863\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.2974 - masked_MSE: 571.2974 - val_loss: 734.6506 - val_masked_MSE: 734.6506\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.2571 - masked_MSE: 571.2571 - val_loss: 734.5887 - val_masked_MSE: 734.5887\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.2154 - masked_MSE: 571.2154 - val_loss: 734.4925 - val_masked_MSE: 734.4926\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.1753 - masked_MSE: 571.1753 - val_loss: 734.4473 - val_masked_MSE: 734.4473\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.1346 - masked_MSE: 571.1346 - val_loss: 734.4804 - val_masked_MSE: 734.4804\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.0939 - masked_MSE: 571.0939 - val_loss: 734.4982 - val_masked_MSE: 734.4982\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.0538 - masked_MSE: 571.0538 - val_loss: 734.4563 - val_masked_MSE: 734.4563\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.0135 - masked_MSE: 571.0135 - val_loss: 734.3846 - val_masked_MSE: 734.3846\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.9734 - masked_MSE: 570.9734 - val_loss: 734.3964 - val_masked_MSE: 734.3964\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.9337 - masked_MSE: 570.9337 - val_loss: 734.3293 - val_masked_MSE: 734.3293\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.8935 - masked_MSE: 570.8935 - val_loss: 734.2814 - val_masked_MSE: 734.2814\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.8538 - masked_MSE: 570.8538 - val_loss: 734.2620 - val_masked_MSE: 734.2620\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.8146 - masked_MSE: 570.8146 - val_loss: 734.2225 - val_masked_MSE: 734.2225\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.7750 - masked_MSE: 570.7750 - val_loss: 734.1751 - val_masked_MSE: 734.1751\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.7357 - masked_MSE: 570.7357 - val_loss: 734.0757 - val_masked_MSE: 734.0757\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.6967 - masked_MSE: 570.6967 - val_loss: 734.0325 - val_masked_MSE: 734.0326\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.6577 - masked_MSE: 570.6577 - val_loss: 733.9919 - val_masked_MSE: 733.9918\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.6187 - masked_MSE: 570.6187 - val_loss: 733.9968 - val_masked_MSE: 733.9968\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.5797 - masked_MSE: 570.5797 - val_loss: 733.9446 - val_masked_MSE: 733.9446\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.5410 - masked_MSE: 570.5410 - val_loss: 733.8633 - val_masked_MSE: 733.8633\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.5024 - masked_MSE: 570.5024 - val_loss: 733.8691 - val_masked_MSE: 733.8691\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.4640 - masked_MSE: 570.4640 - val_loss: 733.8911 - val_masked_MSE: 733.8911\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.4258 - masked_MSE: 570.4258 - val_loss: 733.7805 - val_masked_MSE: 733.7805\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.3873 - masked_MSE: 570.3873 - val_loss: 733.7502 - val_masked_MSE: 733.7502\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.3492 - masked_MSE: 570.3492 - val_loss: 733.6841 - val_masked_MSE: 733.6841\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.3110 - masked_MSE: 570.3110 - val_loss: 733.6098 - val_masked_MSE: 733.6098\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.2733 - masked_MSE: 570.2733 - val_loss: 733.5953 - val_masked_MSE: 733.5953\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.2355 - masked_MSE: 570.2355 - val_loss: 733.5944 - val_masked_MSE: 733.5944\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.1974 - masked_MSE: 570.1974 - val_loss: 733.6068 - val_masked_MSE: 733.6068\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.1603 - masked_MSE: 570.1603 - val_loss: 733.5611 - val_masked_MSE: 733.5611\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.1226 - masked_MSE: 570.1226 - val_loss: 733.4839 - val_masked_MSE: 733.4839\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 570.0851 - masked_MSE: 570.0851 - val_loss: 733.4849 - val_masked_MSE: 733.4849\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.0478 - masked_MSE: 570.0478 - val_loss: 733.4816 - val_masked_MSE: 733.4816\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.0108 - masked_MSE: 570.0108 - val_loss: 733.4240 - val_masked_MSE: 733.4240\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.9736 - masked_MSE: 569.9736 - val_loss: 733.3172 - val_masked_MSE: 733.3173\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.9366 - masked_MSE: 569.9366 - val_loss: 733.3835 - val_masked_MSE: 733.3835\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.8998 - masked_MSE: 569.8998 - val_loss: 733.3602 - val_masked_MSE: 733.3602\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.8630 - masked_MSE: 569.8630 - val_loss: 733.2725 - val_masked_MSE: 733.2725\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.8264 - masked_MSE: 569.8264 - val_loss: 733.1960 - val_masked_MSE: 733.1960\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.7902 - masked_MSE: 569.7902 - val_loss: 733.1749 - val_masked_MSE: 733.1749\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.7542 - masked_MSE: 569.7542 - val_loss: 733.2002 - val_masked_MSE: 733.2002\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.7187 - masked_MSE: 569.7187 - val_loss: 733.2737 - val_masked_MSE: 733.2738\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.6847 - masked_MSE: 569.6847 - val_loss: 733.0941 - val_masked_MSE: 733.0941\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.6522 - masked_MSE: 569.6522 - val_loss: 733.1313 - val_masked_MSE: 733.1313\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.6241 - masked_MSE: 569.6241 - val_loss: 732.9516 - val_masked_MSE: 732.9516\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.6042 - masked_MSE: 569.6042 - val_loss: 733.3273 - val_masked_MSE: 733.3273\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.6009 - masked_MSE: 569.6009 - val_loss: 733.0186 - val_masked_MSE: 733.0186\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.6321 - masked_MSE: 569.6321 - val_loss: 733.6735 - val_masked_MSE: 733.6735\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.7356 - masked_MSE: 569.7356 - val_loss: 733.0731 - val_masked_MSE: 733.0731\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.9883 - masked_MSE: 569.9883 - val_loss: 734.7525 - val_masked_MSE: 734.7525\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.5366 - masked_MSE: 570.5366 - val_loss: 734.3044 - val_masked_MSE: 734.3044\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.6315 - masked_MSE: 571.6315 - val_loss: 738.1973 - val_masked_MSE: 738.1973\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.4930 - masked_MSE: 573.4930 - val_loss: 737.9638 - val_masked_MSE: 737.9638\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.8887 - masked_MSE: 575.8887 - val_loss: 741.6135 - val_masked_MSE: 741.6134\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.0568 - masked_MSE: 577.0568 - val_loss: 736.6223 - val_masked_MSE: 736.6223\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.6335 - masked_MSE: 574.6335 - val_loss: 733.3654 - val_masked_MSE: 733.3654\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.5325 - masked_MSE: 570.5325 - val_loss: 733.3867 - val_masked_MSE: 733.3867\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.5781 - masked_MSE: 569.5781 - val_loss: 734.6578 - val_masked_MSE: 734.6578\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.9797 - masked_MSE: 571.9797 - val_loss: 736.7522 - val_masked_MSE: 736.7522\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 572.5514 - masked_MSE: 572.5514 - val_loss: 732.9232 - val_masked_MSE: 732.9232\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.1390 - masked_MSE: 570.1390 - val_loss: 731.9514 - val_masked_MSE: 731.9514\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.3925 - masked_MSE: 569.3925 - val_loss: 735.0015 - val_masked_MSE: 735.0015\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.0488 - masked_MSE: 571.0488 - val_loss: 734.0823 - val_masked_MSE: 734.0823\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 571.0047 - masked_MSE: 571.0047 - val_loss: 732.7673 - val_masked_MSE: 732.7673\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.2869 - masked_MSE: 569.2869 - val_loss: 733.2536 - val_masked_MSE: 733.2536\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.4542 - masked_MSE: 569.4542 - val_loss: 733.8538 - val_masked_MSE: 733.8539\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 570.5430 - masked_MSE: 570.5430 - val_loss: 734.0915 - val_masked_MSE: 734.0915\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.8868 - masked_MSE: 569.8868 - val_loss: 732.4959 - val_masked_MSE: 732.4960\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.9395 - masked_MSE: 568.9395 - val_loss: 733.3786 - val_masked_MSE: 733.3785\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.4751 - masked_MSE: 569.4751 - val_loss: 734.1958 - val_masked_MSE: 734.1958\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.8690 - masked_MSE: 569.8690 - val_loss: 732.5928 - val_masked_MSE: 732.5928\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.1532 - masked_MSE: 569.1532 - val_loss: 732.9739 - val_masked_MSE: 732.9740\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.8519 - masked_MSE: 568.8519 - val_loss: 733.3773 - val_masked_MSE: 733.3773\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.3245 - masked_MSE: 569.3245 - val_loss: 733.1979 - val_masked_MSE: 733.1979\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 569.2933 - masked_MSE: 569.2933 - val_loss: 732.6221 - val_masked_MSE: 732.6221\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.7873 - masked_MSE: 568.7873 - val_loss: 732.2689 - val_masked_MSE: 732.2689\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.6799 - masked_MSE: 568.6799 - val_loss: 733.0950 - val_masked_MSE: 733.0950\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.9872 - masked_MSE: 568.9872 - val_loss: 732.7737 - val_masked_MSE: 732.7737\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.9736 - masked_MSE: 568.9736 - val_loss: 732.0532 - val_masked_MSE: 732.0532\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.6151 - masked_MSE: 568.6151 - val_loss: 732.0905 - val_masked_MSE: 732.0905\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.4943 - masked_MSE: 568.4943 - val_loss: 732.1409 - val_masked_MSE: 732.1410\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.6833 - masked_MSE: 568.6833 - val_loss: 732.3282 - val_masked_MSE: 732.3282\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.7114 - masked_MSE: 568.7114 - val_loss: 731.9822 - val_masked_MSE: 731.9822\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.4760 - masked_MSE: 568.4760 - val_loss: 731.7807 - val_masked_MSE: 731.7807\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.3356 - masked_MSE: 568.3356 - val_loss: 732.2226 - val_masked_MSE: 732.2226\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.4306 - masked_MSE: 568.4306 - val_loss: 732.0562 - val_masked_MSE: 732.0562\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.4841 - masked_MSE: 568.4841 - val_loss: 731.9035 - val_masked_MSE: 731.9035\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.3547 - masked_MSE: 568.3547 - val_loss: 731.8183 - val_masked_MSE: 731.8183\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.2108 - masked_MSE: 568.2108 - val_loss: 731.6204 - val_masked_MSE: 731.6205\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.2161 - masked_MSE: 568.2161 - val_loss: 731.9651 - val_masked_MSE: 731.9652\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.2708 - masked_MSE: 568.2708 - val_loss: 731.6770 - val_masked_MSE: 731.6770\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.2219 - masked_MSE: 568.2219 - val_loss: 731.6611 - val_masked_MSE: 731.6611\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.1063 - masked_MSE: 568.1063 - val_loss: 731.8973 - val_masked_MSE: 731.8973\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.0501 - masked_MSE: 568.0501 - val_loss: 731.5857 - val_masked_MSE: 731.5857\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.0703 - masked_MSE: 568.0703 - val_loss: 731.7354 - val_masked_MSE: 731.7354\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.0701 - masked_MSE: 568.0701 - val_loss: 731.4881 - val_masked_MSE: 731.4882\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.0012 - masked_MSE: 568.0012 - val_loss: 731.5168 - val_masked_MSE: 731.5167\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.9260 - masked_MSE: 567.9260 - val_loss: 731.7258 - val_masked_MSE: 731.7258\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.9002 - masked_MSE: 567.9002 - val_loss: 731.3162 - val_masked_MSE: 731.3163\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.9022 - masked_MSE: 567.9022 - val_loss: 731.5347 - val_masked_MSE: 731.5347\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.8780 - masked_MSE: 567.8780 - val_loss: 731.4998 - val_masked_MSE: 731.4999\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.8204 - masked_MSE: 567.8204 - val_loss: 731.4385 - val_masked_MSE: 731.4385\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.7680 - masked_MSE: 567.7680 - val_loss: 731.5438 - val_masked_MSE: 731.5438\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.7449 - masked_MSE: 567.7449 - val_loss: 731.0837 - val_masked_MSE: 731.0837\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.7336 - masked_MSE: 567.7336 - val_loss: 731.3433 - val_masked_MSE: 731.3433\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.7057 - masked_MSE: 567.7057 - val_loss: 731.2729 - val_masked_MSE: 731.2729\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.6588 - masked_MSE: 567.6588 - val_loss: 731.1902 - val_masked_MSE: 731.1903\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.6154 - masked_MSE: 567.6154 - val_loss: 731.2554 - val_masked_MSE: 731.2554\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.5890 - masked_MSE: 567.5890 - val_loss: 731.0144 - val_masked_MSE: 731.0144\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.5707 - masked_MSE: 567.5707 - val_loss: 731.3530 - val_masked_MSE: 731.3530\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.5448 - masked_MSE: 567.5448 - val_loss: 731.1097 - val_masked_MSE: 731.1097\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.5070 - masked_MSE: 567.5070 - val_loss: 731.0500 - val_masked_MSE: 731.0500\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.4666 - masked_MSE: 567.4666 - val_loss: 731.0310 - val_masked_MSE: 731.0310\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.4360 - masked_MSE: 567.4360 - val_loss: 730.8983 - val_masked_MSE: 730.8983\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.4130 - masked_MSE: 567.4130 - val_loss: 731.1813 - val_masked_MSE: 731.1813\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.3904 - masked_MSE: 567.3904 - val_loss: 730.8896 - val_masked_MSE: 730.8896\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.3599 - masked_MSE: 567.3599 - val_loss: 730.9518 - val_masked_MSE: 730.9518\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.3243 - masked_MSE: 567.3243 - val_loss: 730.7621 - val_masked_MSE: 730.7621\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.2897 - masked_MSE: 567.2897 - val_loss: 730.7863 - val_masked_MSE: 730.7863\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.2611 - masked_MSE: 567.2611 - val_loss: 730.9045 - val_masked_MSE: 730.9045\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.2365 - masked_MSE: 567.2365 - val_loss: 730.6580 - val_masked_MSE: 730.6580\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.2108 - masked_MSE: 567.2108 - val_loss: 730.7758 - val_masked_MSE: 730.7758\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.1823 - masked_MSE: 567.1823 - val_loss: 730.5270 - val_masked_MSE: 730.5270\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.1504 - masked_MSE: 567.1504 - val_loss: 730.6892 - val_masked_MSE: 730.6892\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.1189 - masked_MSE: 567.1189 - val_loss: 730.5948 - val_masked_MSE: 730.5948\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.0896 - masked_MSE: 567.0896 - val_loss: 730.4922 - val_masked_MSE: 730.4922\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.0627 - masked_MSE: 567.0627 - val_loss: 730.5219 - val_masked_MSE: 730.5219\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.0364 - masked_MSE: 567.0364 - val_loss: 730.5041 - val_masked_MSE: 730.5041\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.0087 - masked_MSE: 567.0087 - val_loss: 730.5571 - val_masked_MSE: 730.5571\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.9796 - masked_MSE: 566.9796 - val_loss: 730.2955 - val_masked_MSE: 730.2955\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.9499 - masked_MSE: 566.9499 - val_loss: 730.3401 - val_masked_MSE: 730.3402\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.9210 - masked_MSE: 566.9210 - val_loss: 730.3389 - val_masked_MSE: 730.3389\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.8928 - masked_MSE: 566.8928 - val_loss: 730.4421 - val_masked_MSE: 730.4421\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.8656 - masked_MSE: 566.8656 - val_loss: 730.3335 - val_masked_MSE: 730.3335\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.8386 - masked_MSE: 566.8386 - val_loss: 730.2363 - val_masked_MSE: 730.2363\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.8112 - masked_MSE: 566.8112 - val_loss: 730.2454 - val_masked_MSE: 730.2455\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.7828 - masked_MSE: 566.7828 - val_loss: 730.2897 - val_masked_MSE: 730.2897\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.7544 - masked_MSE: 566.7544 - val_loss: 730.2552 - val_masked_MSE: 730.2552\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.7258 - masked_MSE: 566.7258 - val_loss: 730.1717 - val_masked_MSE: 730.1717\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.6979 - masked_MSE: 566.6979 - val_loss: 730.1174 - val_masked_MSE: 730.1174\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.6705 - masked_MSE: 566.6705 - val_loss: 730.1828 - val_masked_MSE: 730.1827\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.6432 - masked_MSE: 566.6432 - val_loss: 730.0220 - val_masked_MSE: 730.0220\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.6164 - masked_MSE: 566.6164 - val_loss: 730.0146 - val_masked_MSE: 730.0146\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 566.5893 - masked_MSE: 566.5893 - val_loss: 729.9666 - val_masked_MSE: 729.9666\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.5620 - masked_MSE: 566.5620 - val_loss: 730.0533 - val_masked_MSE: 730.0533\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.5346 - masked_MSE: 566.5346 - val_loss: 729.9101 - val_masked_MSE: 729.9101\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.5071 - masked_MSE: 566.5071 - val_loss: 729.8379 - val_masked_MSE: 729.8379\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.4796 - masked_MSE: 566.4796 - val_loss: 729.7409 - val_masked_MSE: 729.7409\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.4525 - masked_MSE: 566.4525 - val_loss: 729.8264 - val_masked_MSE: 729.8264\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.4254 - masked_MSE: 566.4254 - val_loss: 729.7703 - val_masked_MSE: 729.7703\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.3990 - masked_MSE: 566.3990 - val_loss: 729.7598 - val_masked_MSE: 729.7598\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.3727 - masked_MSE: 566.3727 - val_loss: 729.5568 - val_masked_MSE: 729.5568\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.3466 - masked_MSE: 566.3466 - val_loss: 729.6204 - val_masked_MSE: 729.6204\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.3209 - masked_MSE: 566.3209 - val_loss: 729.4562 - val_masked_MSE: 729.4562\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.2959 - masked_MSE: 566.2959 - val_loss: 729.6369 - val_masked_MSE: 729.6369\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.2719 - masked_MSE: 566.2719 - val_loss: 729.3846 - val_masked_MSE: 729.3846\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.2507 - masked_MSE: 566.2507 - val_loss: 729.7493 - val_masked_MSE: 729.7493\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.2344 - masked_MSE: 566.2344 - val_loss: 729.1709 - val_masked_MSE: 729.1709\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.2260 - masked_MSE: 566.2260 - val_loss: 729.8145 - val_masked_MSE: 729.8145\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.2369 - masked_MSE: 566.2369 - val_loss: 728.7961 - val_masked_MSE: 728.7960\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.2739 - masked_MSE: 566.2739 - val_loss: 730.1807 - val_masked_MSE: 730.1807\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.3766 - masked_MSE: 566.3766 - val_loss: 728.6004 - val_masked_MSE: 728.6005\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.5190 - masked_MSE: 566.5190 - val_loss: 731.0833 - val_masked_MSE: 731.0833\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.7972 - masked_MSE: 566.7972 - val_loss: 728.6169 - val_masked_MSE: 728.6168\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.9732 - masked_MSE: 566.9732 - val_loss: 731.6509 - val_masked_MSE: 731.6509\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.2128 - masked_MSE: 567.2128 - val_loss: 729.2512 - val_masked_MSE: 729.2512\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.2953 - masked_MSE: 567.2953 - val_loss: 731.1385 - val_masked_MSE: 731.1385\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.4969 - masked_MSE: 567.4969 - val_loss: 730.5059 - val_masked_MSE: 730.5059\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.8107 - masked_MSE: 567.8107 - val_loss: 730.8132 - val_masked_MSE: 730.8132\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 568.0493 - masked_MSE: 568.0493 - val_loss: 731.0417 - val_masked_MSE: 731.0416\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.9491 - masked_MSE: 567.9491 - val_loss: 730.0417 - val_masked_MSE: 730.0416\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.3797 - masked_MSE: 567.3797 - val_loss: 729.5010 - val_masked_MSE: 729.5010\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.6055 - masked_MSE: 566.6055 - val_loss: 729.4100 - val_masked_MSE: 729.4100\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.0351 - masked_MSE: 566.0351 - val_loss: 728.4754 - val_masked_MSE: 728.4755\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.8985 - masked_MSE: 565.8985 - val_loss: 729.8787 - val_masked_MSE: 729.8788\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.1086 - masked_MSE: 566.1086 - val_loss: 728.7806 - val_masked_MSE: 728.7806\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.3262 - masked_MSE: 566.3262 - val_loss: 729.8765 - val_masked_MSE: 729.8765\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.3476 - masked_MSE: 566.3476 - val_loss: 729.3845 - val_masked_MSE: 729.3845\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.1810 - masked_MSE: 566.1810 - val_loss: 728.7624 - val_masked_MSE: 728.7624\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.0057 - masked_MSE: 566.0057 - val_loss: 729.9249 - val_masked_MSE: 729.9249\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.9718 - masked_MSE: 565.9718 - val_loss: 727.9246 - val_masked_MSE: 727.9247\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.0222 - masked_MSE: 566.0222 - val_loss: 730.1950 - val_masked_MSE: 730.1949\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.0455 - masked_MSE: 566.0455 - val_loss: 727.8915 - val_masked_MSE: 727.8915\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.8583 - masked_MSE: 565.8583 - val_loss: 729.0832 - val_masked_MSE: 729.0832\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.6347 - masked_MSE: 565.6347 - val_loss: 728.5978 - val_masked_MSE: 728.5978\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.5246 - masked_MSE: 565.5246 - val_loss: 728.0383 - val_masked_MSE: 728.0383\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.5909 - masked_MSE: 565.5909 - val_loss: 729.6057 - val_masked_MSE: 729.6057\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.7242 - masked_MSE: 565.7242 - val_loss: 727.6582 - val_masked_MSE: 727.6582\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.7407 - masked_MSE: 565.7407 - val_loss: 729.2936 - val_masked_MSE: 729.2936\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.6441 - masked_MSE: 565.6441 - val_loss: 727.8182 - val_masked_MSE: 727.8182\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.4945 - masked_MSE: 565.4945 - val_loss: 728.3899 - val_masked_MSE: 728.3899\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.4182 - masked_MSE: 565.4182 - val_loss: 728.5179 - val_masked_MSE: 728.5179\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.4289 - masked_MSE: 565.4289 - val_loss: 727.8870 - val_masked_MSE: 727.8870\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.4573 - masked_MSE: 565.4573 - val_loss: 728.8848 - val_masked_MSE: 728.8848\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.4351 - masked_MSE: 565.4351 - val_loss: 727.8229 - val_masked_MSE: 727.8229\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.3576 - masked_MSE: 565.3576 - val_loss: 728.5142 - val_masked_MSE: 728.5142\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.2731 - masked_MSE: 565.2731 - val_loss: 728.0010 - val_masked_MSE: 728.0010\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.2238 - masked_MSE: 565.2238 - val_loss: 728.0793 - val_masked_MSE: 728.0793\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.2245 - masked_MSE: 565.2245 - val_loss: 728.4031 - val_masked_MSE: 728.4031\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.2484 - masked_MSE: 565.2484 - val_loss: 728.0358 - val_masked_MSE: 728.0358\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.2599 - masked_MSE: 565.2599 - val_loss: 728.3830 - val_masked_MSE: 728.3829\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.2427 - masked_MSE: 565.2427 - val_loss: 728.0681 - val_masked_MSE: 728.0681\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.2067 - masked_MSE: 565.2067 - val_loss: 727.9698 - val_masked_MSE: 727.9698\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.1741 - masked_MSE: 565.1741 - val_loss: 728.4589 - val_masked_MSE: 728.4589\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.1678 - masked_MSE: 565.1678 - val_loss: 727.6268 - val_masked_MSE: 727.6268\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 565.1961 - masked_MSE: 565.1961 - val_loss: 728.9743 - val_masked_MSE: 728.9742\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.2593 - masked_MSE: 565.2593 - val_loss: 727.4988 - val_masked_MSE: 727.4988\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.3354 - masked_MSE: 565.3354 - val_loss: 729.4249 - val_masked_MSE: 729.4249\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.4426 - masked_MSE: 565.4426 - val_loss: 727.5975 - val_masked_MSE: 727.5975\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.5500 - masked_MSE: 565.5500 - val_loss: 729.6013 - val_masked_MSE: 729.6013\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.7242 - masked_MSE: 565.7242 - val_loss: 728.0568 - val_masked_MSE: 728.0568\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.9533 - masked_MSE: 565.9533 - val_loss: 729.9025 - val_masked_MSE: 729.9025\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.2792 - masked_MSE: 566.2792 - val_loss: 728.8957 - val_masked_MSE: 728.8956\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.6422 - masked_MSE: 566.6422 - val_loss: 730.4423 - val_masked_MSE: 730.4423\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.9564 - masked_MSE: 566.9564 - val_loss: 729.4641 - val_masked_MSE: 729.4641\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.0728 - masked_MSE: 567.0728 - val_loss: 730.3833 - val_masked_MSE: 730.3833\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.8605 - masked_MSE: 566.8605 - val_loss: 728.6867 - val_masked_MSE: 728.6867\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.2996 - masked_MSE: 566.2996 - val_loss: 729.0443 - val_masked_MSE: 729.0443\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.5821 - masked_MSE: 565.5821 - val_loss: 727.3220 - val_masked_MSE: 727.3219\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.9837 - masked_MSE: 564.9837 - val_loss: 727.9647 - val_masked_MSE: 727.9647\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.7309 - masked_MSE: 564.7309 - val_loss: 727.5471 - val_masked_MSE: 727.5470\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.8262 - masked_MSE: 564.8262 - val_loss: 728.0482 - val_masked_MSE: 728.0482\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.0917 - masked_MSE: 565.0917 - val_loss: 728.6697 - val_masked_MSE: 728.6697\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.3086 - masked_MSE: 565.3086 - val_loss: 727.8867 - val_masked_MSE: 727.8867\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.3473 - masked_MSE: 565.3473 - val_loss: 729.0573 - val_masked_MSE: 729.0573\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.2234 - masked_MSE: 565.2234 - val_loss: 727.1725 - val_masked_MSE: 727.1725\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.0062 - masked_MSE: 565.0062 - val_loss: 728.7960 - val_masked_MSE: 728.7960\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.8202 - masked_MSE: 564.8202 - val_loss: 726.9777 - val_masked_MSE: 726.9777\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.6840 - masked_MSE: 564.6840 - val_loss: 728.3155 - val_masked_MSE: 728.3155\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.6241 - masked_MSE: 564.6241 - val_loss: 727.5627 - val_masked_MSE: 727.5627\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.6129 - masked_MSE: 564.6129 - val_loss: 727.6614 - val_masked_MSE: 727.6614\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.6272 - masked_MSE: 564.6272 - val_loss: 728.2259 - val_masked_MSE: 728.2259\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.6506 - masked_MSE: 564.6506 - val_loss: 727.0037 - val_masked_MSE: 727.0037\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.6652 - masked_MSE: 564.6652 - val_loss: 728.5156 - val_masked_MSE: 728.5156\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.6619 - masked_MSE: 564.6619 - val_loss: 726.6282 - val_masked_MSE: 726.6283\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.6102 - masked_MSE: 564.6102 - val_loss: 728.3210 - val_masked_MSE: 728.3210\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.5291 - masked_MSE: 564.5291 - val_loss: 726.7702 - val_masked_MSE: 726.7701\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.4211 - masked_MSE: 564.4211 - val_loss: 727.7482 - val_masked_MSE: 727.7482\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.3274 - masked_MSE: 564.3274 - val_loss: 727.2173 - val_masked_MSE: 727.2173\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.2698 - masked_MSE: 564.2698 - val_loss: 727.1816 - val_masked_MSE: 727.1816\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.2562 - masked_MSE: 564.2562 - val_loss: 727.7964 - val_masked_MSE: 727.7964\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.2763 - masked_MSE: 564.2763 - val_loss: 726.8185 - val_masked_MSE: 726.8185\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.3071 - masked_MSE: 564.3071 - val_loss: 728.1657 - val_masked_MSE: 728.1656\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.3314 - masked_MSE: 564.3314 - val_loss: 726.6902 - val_masked_MSE: 726.6903\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.3300 - masked_MSE: 564.3300 - val_loss: 728.1568 - val_masked_MSE: 728.1568\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.3088 - masked_MSE: 564.3088 - val_loss: 726.6422 - val_masked_MSE: 726.6422\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.2640 - masked_MSE: 564.2640 - val_loss: 727.7441 - val_masked_MSE: 727.7441\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.2137 - masked_MSE: 564.2137 - val_loss: 726.6939 - val_masked_MSE: 726.6939\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.1630 - masked_MSE: 564.1630 - val_loss: 727.2839 - val_masked_MSE: 727.2838\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.1221 - masked_MSE: 564.1221 - val_loss: 726.9035 - val_masked_MSE: 726.9035\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.0911 - masked_MSE: 564.0911 - val_loss: 727.0733 - val_masked_MSE: 727.0733\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.0663 - masked_MSE: 564.0663 - val_loss: 727.2029 - val_masked_MSE: 727.2030\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.0427 - masked_MSE: 564.0427 - val_loss: 727.0279 - val_masked_MSE: 727.0279\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.0166 - masked_MSE: 564.0166 - val_loss: 727.2693 - val_masked_MSE: 727.2694\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.9865 - masked_MSE: 563.9865 - val_loss: 726.9036 - val_masked_MSE: 726.9037\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 563.9532 - masked_MSE: 563.9532 - val_loss: 727.0143 - val_masked_MSE: 727.0143\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.9197 - masked_MSE: 563.9197 - val_loss: 726.8682 - val_masked_MSE: 726.8682\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.8878 - masked_MSE: 563.8878 - val_loss: 726.8580 - val_masked_MSE: 726.8580\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.8597 - masked_MSE: 563.8597 - val_loss: 727.0106 - val_masked_MSE: 727.0106\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.8364 - masked_MSE: 563.8364 - val_loss: 726.6962 - val_masked_MSE: 726.6962\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.8191 - masked_MSE: 563.8191 - val_loss: 727.1569 - val_masked_MSE: 727.1569\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.8087 - masked_MSE: 563.8087 - val_loss: 726.4981 - val_masked_MSE: 726.4981\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.8061 - masked_MSE: 563.8061 - val_loss: 727.3340 - val_masked_MSE: 727.3340\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.8158 - masked_MSE: 563.8158 - val_loss: 726.2651 - val_masked_MSE: 726.2651\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.8418 - masked_MSE: 563.8418 - val_loss: 727.5948 - val_masked_MSE: 727.5948\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.8994 - masked_MSE: 563.8994 - val_loss: 726.1059 - val_masked_MSE: 726.1059\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.9963 - masked_MSE: 563.9963 - val_loss: 728.1375 - val_masked_MSE: 728.1375\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.1741 - masked_MSE: 564.1741 - val_loss: 726.0661 - val_masked_MSE: 726.0661\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.4390 - masked_MSE: 564.4390 - val_loss: 729.2242 - val_masked_MSE: 729.2242\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.8735 - masked_MSE: 564.8735 - val_loss: 726.6813 - val_masked_MSE: 726.6813\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.4171 - masked_MSE: 565.4171 - val_loss: 730.7570 - val_masked_MSE: 730.7570\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.1393 - masked_MSE: 566.1393 - val_loss: 727.9582 - val_masked_MSE: 727.9581\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.7761 - masked_MSE: 566.7761 - val_loss: 731.3788 - val_masked_MSE: 731.3788\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.2435 - masked_MSE: 567.2435 - val_loss: 729.3765 - val_masked_MSE: 729.3765\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 567.2054 - masked_MSE: 567.2054 - val_loss: 729.2878 - val_masked_MSE: 729.2877\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.6373 - masked_MSE: 566.6373 - val_loss: 729.7534 - val_masked_MSE: 729.7534\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 565.7941 - masked_MSE: 565.7941 - val_loss: 726.5999 - val_masked_MSE: 726.5999\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.6179 - masked_MSE: 564.6179 - val_loss: 727.6857 - val_masked_MSE: 727.6857\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.9078 - masked_MSE: 563.9078 - val_loss: 724.8081 - val_masked_MSE: 724.8081\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.0781 - masked_MSE: 564.0781 - val_loss: 727.5302 - val_masked_MSE: 727.5302\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.5934 - masked_MSE: 564.5934 - val_loss: 727.3403 - val_masked_MSE: 727.3403\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.8318 - masked_MSE: 564.8318 - val_loss: 727.3854 - val_masked_MSE: 727.3854\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.3807 - masked_MSE: 564.3807 - val_loss: 726.9505 - val_masked_MSE: 726.9505\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.6555 - masked_MSE: 563.6555 - val_loss: 726.0814 - val_masked_MSE: 726.0814\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.3848 - masked_MSE: 563.3848 - val_loss: 727.5103 - val_masked_MSE: 727.5103\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.6971 - masked_MSE: 563.6971 - val_loss: 726.8859 - val_masked_MSE: 726.8859\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.1104 - masked_MSE: 564.1104 - val_loss: 727.9234 - val_masked_MSE: 727.9234\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.1476 - masked_MSE: 564.1476 - val_loss: 725.6116 - val_masked_MSE: 725.6116\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.8582 - masked_MSE: 563.8582 - val_loss: 727.8880 - val_masked_MSE: 727.8881\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.7946 - masked_MSE: 563.7946 - val_loss: 724.8391 - val_masked_MSE: 724.8390\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.1834 - masked_MSE: 564.1834 - val_loss: 726.8449 - val_masked_MSE: 726.8448\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.9187 - masked_MSE: 563.9187 - val_loss: 725.7646 - val_masked_MSE: 725.7646\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.5345 - masked_MSE: 563.5345 - val_loss: 726.7766 - val_masked_MSE: 726.7766\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.5121 - masked_MSE: 563.5121 - val_loss: 726.6481 - val_masked_MSE: 726.6481\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.3963 - masked_MSE: 563.3963 - val_loss: 724.3791 - val_masked_MSE: 724.3790\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.3562 - masked_MSE: 563.3562 - val_loss: 725.8649 - val_masked_MSE: 725.8649\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.2017 - masked_MSE: 563.2017 - val_loss: 726.5557 - val_masked_MSE: 726.5557\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.2194 - masked_MSE: 563.2194 - val_loss: 727.4390 - val_masked_MSE: 727.4390\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.2407 - masked_MSE: 563.2407 - val_loss: 726.3951 - val_masked_MSE: 726.3951\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.1854 - masked_MSE: 563.1854 - val_loss: 726.8792 - val_masked_MSE: 726.8793\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.1194 - masked_MSE: 563.1194 - val_loss: 725.6877 - val_masked_MSE: 725.6877\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.1313 - masked_MSE: 563.1313 - val_loss: 727.1110 - val_masked_MSE: 727.1110\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.1876 - masked_MSE: 563.1876 - val_loss: 725.2900 - val_masked_MSE: 725.2900\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.3711 - masked_MSE: 563.3711 - val_loss: 728.1761 - val_masked_MSE: 728.1761\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.6113 - masked_MSE: 563.6113 - val_loss: 724.9180 - val_masked_MSE: 724.9180\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.9425 - masked_MSE: 563.9425 - val_loss: 728.6295 - val_masked_MSE: 728.6295\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.1917 - masked_MSE: 564.1917 - val_loss: 724.3353 - val_masked_MSE: 724.3353\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.0645 - masked_MSE: 564.0645 - val_loss: 725.5975 - val_masked_MSE: 725.5975\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.0657 - masked_MSE: 563.0657 - val_loss: 726.0042 - val_masked_MSE: 726.0042\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.9865 - masked_MSE: 562.9865 - val_loss: 724.9510 - val_masked_MSE: 724.9510\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.6527 - masked_MSE: 563.6527 - val_loss: 727.2993 - val_masked_MSE: 727.2993\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.5663 - masked_MSE: 563.5663 - val_loss: 726.3278 - val_masked_MSE: 726.3278\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.9507 - masked_MSE: 562.9507 - val_loss: 726.5555 - val_masked_MSE: 726.5555\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.4451 - masked_MSE: 563.4451 - val_loss: 730.2773 - val_masked_MSE: 730.2773\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.2111 - masked_MSE: 564.2111 - val_loss: 724.6224 - val_masked_MSE: 724.6224\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.2586 - masked_MSE: 564.2586 - val_loss: 724.3643 - val_masked_MSE: 724.3643\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.8516 - masked_MSE: 562.8516 - val_loss: 729.5588 - val_masked_MSE: 729.5588\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 564.3492 - masked_MSE: 564.3492 - val_loss: 725.6372 - val_masked_MSE: 725.6372\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.6210 - masked_MSE: 566.6210 - val_loss: 725.8444 - val_masked_MSE: 725.8444\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.9786 - masked_MSE: 564.9786 - val_loss: 725.6818 - val_masked_MSE: 725.6818\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.4836 - masked_MSE: 563.4836 - val_loss: 728.9894 - val_masked_MSE: 728.9894\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 566.1591 - masked_MSE: 566.1591 - val_loss: 730.9050 - val_masked_MSE: 730.9049\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.8134 - masked_MSE: 563.8134 - val_loss: 726.7671 - val_masked_MSE: 726.7671\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.7974 - masked_MSE: 563.7974 - val_loss: 723.9756 - val_masked_MSE: 723.9756\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.4054 - masked_MSE: 564.4054 - val_loss: 725.4385 - val_masked_MSE: 725.4385\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.3915 - masked_MSE: 563.3915 - val_loss: 730.0369 - val_masked_MSE: 730.0369\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.1172 - masked_MSE: 564.1172 - val_loss: 725.8773 - val_masked_MSE: 725.8773\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.6873 - masked_MSE: 564.6873 - val_loss: 724.2290 - val_masked_MSE: 724.2290\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.2250 - masked_MSE: 563.2250 - val_loss: 726.7116 - val_masked_MSE: 726.7116\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.9769 - masked_MSE: 563.9769 - val_loss: 726.1738 - val_masked_MSE: 726.1738\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 564.2769 - masked_MSE: 564.2769 - val_loss: 728.5783 - val_masked_MSE: 728.5782\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.1399 - masked_MSE: 563.1399 - val_loss: 727.3705 - val_masked_MSE: 727.3705\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.2546 - masked_MSE: 563.2546 - val_loss: 724.7612 - val_masked_MSE: 724.7612\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.4344 - masked_MSE: 563.4344 - val_loss: 727.7720 - val_masked_MSE: 727.7720\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.0313 - masked_MSE: 563.0313 - val_loss: 728.9026 - val_masked_MSE: 728.9027\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.8536 - masked_MSE: 562.8536 - val_loss: 726.9961 - val_masked_MSE: 726.9961\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.2581 - masked_MSE: 563.2581 - val_loss: 726.4158 - val_masked_MSE: 726.4159\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.6274 - masked_MSE: 562.6274 - val_loss: 726.3318 - val_masked_MSE: 726.3319\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.8134 - masked_MSE: 562.8134 - val_loss: 724.3249 - val_masked_MSE: 724.3248\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.8654 - masked_MSE: 562.8654 - val_loss: 725.0623 - val_masked_MSE: 725.0623\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.5200 - masked_MSE: 562.5200 - val_loss: 725.2344 - val_masked_MSE: 725.2344\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.6370 - masked_MSE: 562.6370 - val_loss: 723.8930 - val_masked_MSE: 723.8930\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.6947 - masked_MSE: 562.6947 - val_loss: 725.6295 - val_masked_MSE: 725.6295\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.3937 - masked_MSE: 562.3937 - val_loss: 727.1706 - val_masked_MSE: 727.1706\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.4738 - masked_MSE: 562.4738 - val_loss: 725.0791 - val_masked_MSE: 725.0791\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.7380 - masked_MSE: 562.7380 - val_loss: 726.9759 - val_masked_MSE: 726.9759\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.7051 - masked_MSE: 562.7051 - val_loss: 725.2200 - val_masked_MSE: 725.2201\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.2062 - masked_MSE: 562.2062 - val_loss: 724.4404 - val_masked_MSE: 724.4404\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.3980 - masked_MSE: 562.3980 - val_loss: 726.1065 - val_masked_MSE: 726.1065\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.5504 - masked_MSE: 562.5504 - val_loss: 723.6623 - val_masked_MSE: 723.6622\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.5411 - masked_MSE: 562.5411 - val_loss: 725.0878 - val_masked_MSE: 725.0878\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.1375 - masked_MSE: 562.1375 - val_loss: 725.4170 - val_masked_MSE: 725.4170\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.1409 - masked_MSE: 562.1409 - val_loss: 723.7718 - val_masked_MSE: 723.7718\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.3811 - masked_MSE: 562.3811 - val_loss: 725.4509 - val_masked_MSE: 725.4509\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.3007 - masked_MSE: 562.3007 - val_loss: 723.6429 - val_masked_MSE: 723.6429\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.1340 - masked_MSE: 562.1340 - val_loss: 725.0735 - val_masked_MSE: 725.0735\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.9573 - masked_MSE: 561.9573 - val_loss: 726.0629 - val_masked_MSE: 726.0629\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.0656 - masked_MSE: 562.0656 - val_loss: 724.3747 - val_masked_MSE: 724.3747\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.0151 - masked_MSE: 562.0151 - val_loss: 724.7239 - val_masked_MSE: 724.7239\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.9207 - masked_MSE: 561.9207 - val_loss: 724.9730 - val_masked_MSE: 724.9730\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.9323 - masked_MSE: 561.9323 - val_loss: 723.8456 - val_masked_MSE: 723.8456\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.0828 - masked_MSE: 562.0828 - val_loss: 725.0400 - val_masked_MSE: 725.0400\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.0523 - masked_MSE: 562.0523 - val_loss: 723.8857 - val_masked_MSE: 723.8857\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.9406 - masked_MSE: 561.9406 - val_loss: 725.7065 - val_masked_MSE: 725.7065\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.8563 - masked_MSE: 561.8563 - val_loss: 725.1453 - val_masked_MSE: 725.1453\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.7970 - masked_MSE: 561.7970 - val_loss: 724.9288 - val_masked_MSE: 724.9288\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.7742 - masked_MSE: 561.7742 - val_loss: 724.6814 - val_masked_MSE: 724.6814\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.8055 - masked_MSE: 561.8055 - val_loss: 724.5754 - val_masked_MSE: 724.5754\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 561.7933 - masked_MSE: 561.7933 - val_loss: 725.2126 - val_masked_MSE: 725.2126\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.7751 - masked_MSE: 561.7751 - val_loss: 724.7432 - val_masked_MSE: 724.7432\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.7104 - masked_MSE: 561.7104 - val_loss: 724.3682 - val_masked_MSE: 724.3682\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.6609 - masked_MSE: 561.6609 - val_loss: 724.5439 - val_masked_MSE: 724.5439\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.6417 - masked_MSE: 561.6417 - val_loss: 724.0037 - val_masked_MSE: 724.0037\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.6286 - masked_MSE: 561.6286 - val_loss: 724.8555 - val_masked_MSE: 724.8555\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.6315 - masked_MSE: 561.6315 - val_loss: 723.9089 - val_masked_MSE: 723.9089\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.6100 - masked_MSE: 561.6100 - val_loss: 724.4605 - val_masked_MSE: 724.4605\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.5807 - masked_MSE: 561.5807 - val_loss: 723.7996 - val_masked_MSE: 723.7996\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.5463 - masked_MSE: 561.5463 - val_loss: 724.7183 - val_masked_MSE: 724.7183\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.5161 - masked_MSE: 561.5161 - val_loss: 724.3867 - val_masked_MSE: 724.3867\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.5026 - masked_MSE: 561.5026 - val_loss: 724.3663 - val_masked_MSE: 724.3663\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.4951 - masked_MSE: 561.4951 - val_loss: 724.2365 - val_masked_MSE: 724.2365\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.4908 - masked_MSE: 561.4908 - val_loss: 724.6858 - val_masked_MSE: 724.6858\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.4831 - masked_MSE: 561.4831 - val_loss: 724.4421 - val_masked_MSE: 724.4421\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.4741 - masked_MSE: 561.4741 - val_loss: 724.8572 - val_masked_MSE: 724.8572\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.4704 - masked_MSE: 561.4704 - val_loss: 724.2274 - val_masked_MSE: 724.2274\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.4783 - masked_MSE: 561.4783 - val_loss: 725.3502 - val_masked_MSE: 725.3502\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.5153 - masked_MSE: 561.5153 - val_loss: 723.8720 - val_masked_MSE: 723.8720\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.5862 - masked_MSE: 561.5862 - val_loss: 725.6221 - val_masked_MSE: 725.6221\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.7203 - masked_MSE: 561.7203 - val_loss: 723.4985 - val_masked_MSE: 723.4985\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 561.9308 - masked_MSE: 561.9308 - val_loss: 726.7586 - val_masked_MSE: 726.7587\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.2686 - masked_MSE: 562.2686 - val_loss: 723.6868 - val_masked_MSE: 723.6868\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.7605 - masked_MSE: 562.7605 - val_loss: 727.6281 - val_masked_MSE: 727.6281\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 563.2081 - masked_MSE: 563.2081 - val_loss: 726.4058 - val_masked_MSE: 726.4058\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 562.9579 - masked_MSE: 562.9579 - val_loss: 741.7598 - val_masked_MSE: 741.7598\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 573.7488 - masked_MSE: 573.7488 - val_loss: 751.5803 - val_masked_MSE: 751.5803\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 621.9868 - masked_MSE: 621.9868 - val_loss: 760.8823 - val_masked_MSE: 760.8823\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 658.1523 - masked_MSE: 658.1523 - val_loss: 734.4953 - val_masked_MSE: 734.4953\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 641.8304 - masked_MSE: 641.8304 - val_loss: 713.2141 - val_masked_MSE: 713.2141\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 615.7990 - masked_MSE: 615.7990 - val_loss: 740.1551 - val_masked_MSE: 740.1551\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 619.1682 - masked_MSE: 619.1682 - val_loss: 767.1837 - val_masked_MSE: 767.1837\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 624.2469 - masked_MSE: 624.2469 - val_loss: 735.1186 - val_masked_MSE: 735.1186\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 595.7000 - masked_MSE: 595.7000 - val_loss: 754.0778 - val_masked_MSE: 754.0778\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 614.8689 - masked_MSE: 614.8689 - val_loss: 736.6812 - val_masked_MSE: 736.6812\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 595.7697 - masked_MSE: 595.7697 - val_loss: 753.1794 - val_masked_MSE: 753.1793\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 603.4409 - masked_MSE: 603.4409 - val_loss: 746.3592 - val_masked_MSE: 746.3592\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 588.0676 - masked_MSE: 588.0676 - val_loss: 759.9059 - val_masked_MSE: 759.9059\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 590.0045 - masked_MSE: 590.0045 - val_loss: 767.8607 - val_masked_MSE: 767.8607\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 586.1439 - masked_MSE: 586.1439 - val_loss: 782.7308 - val_masked_MSE: 782.7308\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 591.8860 - masked_MSE: 591.8860 - val_loss: 765.9079 - val_masked_MSE: 765.9078\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 581.2176 - masked_MSE: 581.2176 - val_loss: 762.0662 - val_masked_MSE: 762.0663\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 585.6107 - masked_MSE: 585.6107 - val_loss: 759.5405 - val_masked_MSE: 759.5405\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 584.3010 - masked_MSE: 584.3010 - val_loss: 763.6750 - val_masked_MSE: 763.6749\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 582.3549 - masked_MSE: 582.3549 - val_loss: 765.3996 - val_masked_MSE: 765.3995\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 578.2410 - masked_MSE: 578.2410 - val_loss: 768.0477 - val_masked_MSE: 768.0477\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.7650 - masked_MSE: 577.7650 - val_loss: 767.5401 - val_masked_MSE: 767.5401\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 577.4799 - masked_MSE: 577.4799 - val_loss: 760.1812 - val_masked_MSE: 760.1812\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 574.5968 - masked_MSE: 574.5968 - val_loss: 754.6790 - val_masked_MSE: 754.6791\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 575.5595 - masked_MSE: 575.5595 - val_loss: 745.9048 - val_masked_MSE: 745.9048\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    verbose=1, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vv1swEsuoSr"
   },
   "outputs": [],
   "source": [
    "# del history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "k3E95CpHSEm8"
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "2evHnZwkVsEM",
    "outputId": "ee458a97-5489-4cfc-e2f8-7a540bb0157a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.ylim([0, 2500])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "lH4EjlMiE7oF",
    "outputId": "e6ca5d18-8e91-4fcb-9fa0-195c73cef132"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_3dd6c306-1e3f-4592-a4dd-d84efcd08c08\", \"model.h5\", 201592)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "files.download('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "7exuZrJCE-8T",
    "outputId": "e72ce8e6-4027-44e5-e94a-01dab64573d6"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-534b7a74019f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtxI9D3_jmHs"
   },
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqsYdsv3EDOt"
   },
   "outputs": [],
   "source": [
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9pYGgJDjnrk"
   },
   "outputs": [],
   "source": [
    "# model = keras.models.load_model('model.h5', custom_objects={'masked_MSE': masked_MSE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "whLYKOtxXYGc"
   },
   "outputs": [],
   "source": [
    "def make_predictions(input):\n",
    "  preds = model.predict(input)\n",
    "  # print(preds)\n",
    "  return preds\n",
    "\n",
    "def download_predictions(preds, i):\n",
    "  np.save(\"pred.npy\", preds[i])\n",
    "  np.save(\"og.npy\", X_train[i])\n",
    "  np.save(\"true.npy\", y_train[i])\n",
    "  files.download('pred.npy')\n",
    "  files.download('og.npy')\n",
    "  files.download('true.npy')\n",
    "\n",
    "  # Apply mask to predictions\n",
    "  # for j, pred in enumerate(preds):\n",
    "    # pred[mask == False] = X_train[j][mask == False][:,:2] # (1-mask)*og + mask*pred\n",
    "  \n",
    "  # np.save(\"pred_masked.npy\", preds[i])\n",
    "  # files.download('pred_masked.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "B3z_QeANexGg",
    "outputId": "265e504e-50e9-419a-a326-18ee1d7fa3ad"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_d084ca2f-d676-4215-99a7-b31da6df5c6e\", \"pred.npy\", 19632128)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eye = np.dstack([np.zeros((1500, 818))]*2)\n",
    "# eye = np.dstack([np.eye(1500, 818)]*2)\n",
    "# print(eye.shape)\n",
    "# pred = model.predict(np.array([eye,]))\n",
    "np.save(\"pred.npy\", X_train[6])\n",
    "files.download('pred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpItE6vngaZI",
    "outputId": "72bef460-82a6-408e-8bbb-b825b567b5ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1500, 818, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "dbGLNd1snayP"
   },
   "outputs": [],
   "source": [
    "preds = make_predictions(X_train)\n",
    "# print(X_train[0].shape)\n",
    "# print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Yg7qs3OPjYMN",
    "outputId": "b5bfd113-1091-459c-f5ba-a9a92afb2255"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_92811af9-2482-44d8-acc6-c583e54ea10a\", \"pred.npy\", 9816128)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_6f3f14fd-6f5a-4ad3-a9ad-5b08cfd954ee\", \"og.npy\", 29448128)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_4b91234a-6eb3-47cc-b65d-8ad8049cc4cb\", \"true.npy\", 19632128)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_predictions(preds, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "xCJnMx5JZ5AF",
    "outputId": "1190dd60-e651-40b8-f96a-bb98a2b9a7db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fabacf86f98>"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preds[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpf77XiuJehI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Tep1LJhKK6tA"
   ],
   "name": "inpaint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
